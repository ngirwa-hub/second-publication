{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc8eb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13200, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"merged_importance_zeroshot.csv\")\n",
    "\n",
    "#drop source human\n",
    "df = df[df['source'] != 'human']\n",
    "\n",
    "\n",
    "#save to csv\n",
    "df.to_csv(\"importance_zeroshot.csv\", index=False)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6368997d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: merged_importance_zero_context.csv rows: 26400\n"
     ]
    }
   ],
   "source": [
    "#concatnate two datasets\n",
    "import pandas as pd\n",
    "\n",
    "zero  = \"importance_zeroshot.csv\"\n",
    "context  = \"importance_context.csv\"\n",
    "OUT_FILE = \"merged_importance_zero_context.csv\"\n",
    "\n",
    "# load\n",
    "zero = pd.read_csv(zero)\n",
    "context = pd.read_csv(context)\n",
    "\n",
    "# keep exactly these shared columns (in this order)\n",
    "cols = [\n",
    "    'row_id','source','base_model','variant_id','model',\n",
    "    'dc_solution','rating','label','condition','iteration','timestamp'\n",
    "]\n",
    "\n",
    "# some columns may be missing in human (base_model/variant_id/model/iteration/timestamp) → create if needed\n",
    "for c in cols:\n",
    "    if c not in zero.columns: zero[c] = pd.NA\n",
    "    if c not in context.columns: context[c] = pd.NA\n",
    "\n",
    "merged = pd.concat([context[cols], zero[cols]], ignore_index=True)\n",
    "\n",
    "merged.to_csv(OUT_FILE, index=False)\n",
    "print(\"Saved:\", OUT_FILE, \"rows:\", len(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b434c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"merged_importance_zero_context.csv\")\n",
    "\n",
    "#rename value in column 'base_model' from 'gemma3-12b' to 'gemma3', and 'llama-pro' to 'llama'\n",
    "df['base_model'] = df['base_model'].replace({'gemma': 'gemma3', 'llama-pro': 'llama'})\n",
    "\n",
    "\n",
    "#SAVE TO CSV\n",
    "df.to_csv(\"merged_importance_zero_context.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b187b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(\"your_file.csv\")\n",
    "pattern = r'^(?P<head>[^_\\n]*?)-[^_\\n]+(?=_)'\n",
    "repl    = r'\\g<head>'\n",
    "\n",
    "df[\"variant_id\"] = df[\"variant_id\"].astype(str).str.replace(pattern, repl, regex=True)\n",
    "df[\"row_id\"]     = df[\"row_id\"].astype(str).str.replace(pattern, repl, regex=True)\n",
    "\n",
    "df.to_csv(\"new_merged_importance_zero_context.csv\", index=False)\n",
    "#this new_merged...csv, came into use due to challenge of naming that happened in two base models: gemma3, and llama\n",
    "#therefore this is the correct version to use for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ee5a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['phi4' 'llama' 'mistral' 'gemma3']\n",
      "['CONTEXT' 'ZEROSHOT']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"new_merged_importance_zero_context.csv\")\n",
    "\n",
    "#select distinct values in column 'base_model'\n",
    "df['base_model'].unique()\n",
    "df['condition'].unique()\n",
    "\n",
    "print(df['base_model'].unique())\n",
    "print(df['condition'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac9eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing distributions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"new_merged_importance_zero_context.csv\")\n",
    "\n",
    "needed = {\"base_model\",\"variant_id\",\"dc_solution\",\"rating\",\"condition\",\"iteration\",\"timestamp\"}\n",
    "missing = needed - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "df[\"condition\"] = df[\"condition\"].str.upper().str.replace(\" \", \"\")\n",
    "df = df[df[\"condition\"].isin([\"ZEROSHOT\",\"CONTEXT\"])].copy()\n",
    "\n",
    "df[\"rating\"] = pd.to_numeric(df[\"rating\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df = df[df[\"rating\"].between(0,4)].copy()\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "# Keep the last occurrence if duplicates for the same key\n",
    "df = (df.sort_values(\"timestamp\")\n",
    "        .groupby([\"base_model\",\"variant_id\",\"dc_solution\",\"condition\",\"iteration\"], as_index=False)\n",
    "        .tail(1))\n",
    "\n",
    "# ===================== PAIRED BY ITERATION =====================\n",
    "pairs = (df.pivot_table(\n",
    "            index=[\"base_model\",\"variant_id\",\"dc_solution\",\"iteration\"],\n",
    "            columns=\"condition\",\n",
    "            values=\"rating\",\n",
    "            aggfunc=\"first\")\n",
    "         .reset_index())\n",
    "\n",
    "pairs = pairs.dropna(subset=[\"ZEROSHOT\",\"CONTEXT\"]).copy()\n",
    "pairs[\"delta\"] = pairs[\"CONTEXT\"] - pairs[\"ZEROSHOT\"]\n",
    "\n",
    "# Diagnostics (expect 50 pairs per base_model • variant • dc_solution)\n",
    "expected_per_solution = 50\n",
    "coverage = (pairs.groupby([\"base_model\",\"variant_id\",\"dc_solution\"])[\"delta\"]\n",
    "                 .size().rename(\"n_pairs\").reset_index())\n",
    "coverage[\"coverage_ok\"] = coverage[\"n_pairs\"].eq(expected_per_solution)\n",
    "print(\"\\n=== Coverage sample ===\")\n",
    "print(coverage.head())\n",
    "\n",
    "# ===================== SUMMARY HELPERS (MEDIANS + proportions) =====================\n",
    "def summarize_median(paired_df, keys):\n",
    "    g = paired_df.copy()\n",
    "    # proportions of sign changes (paired)\n",
    "    change = np.select([g[\"delta\"]>0, g[\"delta\"]<0], [\"improved\",\"not improved\"], default=\"same\")\n",
    "    g[\"change\"] = change\n",
    "    props = (g.groupby(keys + [\"change\"]).size()\n",
    "               .unstack(fill_value=0))\n",
    "    props = props.div(props.sum(axis=1), axis=0)\n",
    "    props = props.rename(columns=lambda c: f\"prop_{c}\")\n",
    "\n",
    "    # medians (per condition) and median delta\n",
    "    med = (g.groupby(keys)\n",
    "             .agg(median_zeroshot=(\"ZEROSHOT\",\"median\"),\n",
    "                  median_context=(\"CONTEXT\",\"median\"),\n",
    "                  median_delta=(\"delta\",\"median\"),\n",
    "                  n_pairs=(\"delta\",\"size\"))\n",
    "             .reset_index())\n",
    "    res = med.merge(props.reset_index(), on=keys, how=\"left\")\n",
    "    for col in [\"prop_improved\",\"prop_same\",\"prop_not_improved\"]:\n",
    "        if col not in res:\n",
    "            res[col] = 0.0\n",
    "    return res\n",
    "\n",
    "# ---------- (A) BY BASE MODEL ----------\n",
    "by_bm = summarize_median(pairs, [\"base_model\"]).sort_values(\"base_model\")\n",
    "print(\"\\n=== Paired summary (Medians) — BY BASE MODEL ===\")\n",
    "print(by_bm[[\"base_model\",\"n_pairs\",\"median_zeroshot\",\"median_context\",\"median_delta\",\n",
    "             \"prop_improved\",\"prop_same\",\"prop_not_improved\"]])\n",
    "\n",
    "# ---------- (B) BY DC SOLUTION (across all base models & variants) ----------\n",
    "by_solution = summarize_median(pairs, [\"dc_solution\"]).sort_values(\"dc_solution\")\n",
    "print(\"\\n=== Paired summary (Medians) — BY DC SOLUTION ===\")\n",
    "print(by_solution[[\"dc_solution\",\"n_pairs\",\"median_zeroshot\",\"median_context\",\"median_delta\",\n",
    "                   \"prop_improved\",\"prop_same\",\"prop_not_improved\"]])\n",
    "\n",
    "# (Optional) If you’d also like base_model × dc_solution:\n",
    "# by_bm_sol = summarize_median(pairs, [\"base_model\",\"dc_solution\"])\n",
    "\n",
    "# Stacked proportions 0–4 per condition\n",
    "def stacked_props(data, group_cols, title):\n",
    "    rating_levels = [0,1,2,3,4]\n",
    "    ct = (data.groupby(group_cols + [\"rating\"])\n",
    "              .size()\n",
    "              .unstack(fill_value=0)\n",
    "              .reindex(columns=rating_levels, fill_value=0))\n",
    "    prop = ct.div(ct.sum(axis=1), axis=0)\n",
    "    ax = prop.plot(kind=\"bar\", stacked=True, figsize=(9,4))\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\" • \".join(group_cols))\n",
    "    ax.set_ylabel(\"Proportion\")\n",
    "    ax.legend(title=\"Rating\", bbox_to_anchor=(1.02,1), loc=\"upper left\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# Per BASE MODEL\n",
    "for bm, g in df.groupby(\"base_model\"):\n",
    "    stacked_props(g, [\"condition\"], f\"Rating distribution (0–4) — {bm} (all variants, solutions, iterations)\")\n",
    "\n",
    "# Per DC SOLUTION\n",
    "for sol, g in df.groupby(\"dc_solution\"):\n",
    "    stacked_props(g, [\"condition\"], f\"Rating distribution (0–4) — {sol} (all base models, variants, iterations)\")\n",
    "\n",
    "# ECDF unpaired view\n",
    "def plot_ecdf(series, label):\n",
    "    x = np.sort(series.astype(float))\n",
    "    y = np.arange(1, len(x)+1) / len(x)\n",
    "    plt.step(x, y, where=\"post\", label=label)\n",
    "\n",
    "# ECDF per BASE MODEL\n",
    "for bm, g in df.groupby(\"base_model\"):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    for cond, gg in g.groupby(\"condition\"):\n",
    "        plot_ecdf(gg[\"rating\"].values, cond)\n",
    "    plt.title(f\"ECDF — {bm}\")\n",
    "    plt.xlabel(\"Rating (0–4)\"); plt.ylabel(\"Proportion ≤ rating\")\n",
    "    plt.legend(title=\"Condition\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ECDF per DC SOLUTION\n",
    "for sol, g in df.groupby(\"dc_solution\"):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    for cond, gg in g.groupby(\"condition\"):\n",
    "        plot_ecdf(gg[\"rating\"].values, cond)\n",
    "    plt.title(f\"ECDF — {sol}\")\n",
    "    plt.xlabel(\"Rating (0–4)\"); plt.ylabel(\"Proportion ≤ rating\")\n",
    "    plt.legend(title=\"Condition\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Unpaired comparison (CONTEXT vs ZEROSHOT) by base model and by dc solution.\n",
    "def cliffs_delta_unpaired(x, y):\n",
    "    x = np.asarray(x); y = np.asarray(y)\n",
    "    gt = sum((xi > y).sum() for xi in x)\n",
    "    lt = sum((xi < y).sum() for xi in x)\n",
    "    n = len(x)*len(y)\n",
    "    return (gt - lt) / n if n else np.nan\n",
    "\n",
    "def cliff_by(df, keys):\n",
    "    rows = []\n",
    "    for key_vals, g in df.groupby(keys):\n",
    "        a = g[g[\"condition\"]==\"ZEROSHOT\"][\"rating\"].values\n",
    "        b = g[g[\"condition\"]==\"CONTEXT\"][\"rating\"].values\n",
    "        rows.append({**({k:v for k,v in zip(keys, key_vals)} if isinstance(key_vals, tuple) else {keys[0]: key_vals}),\n",
    "                     \"cliffs_delta\": cliffs_delta_unpaired(b, a)})  # positive => CONTEXT higher\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "cliff_bm  = cliff_by(df, [\"base_model\"]).sort_values(\"base_model\")\n",
    "cliff_sol = cliff_by(df, [\"dc_solution\"]).sort_values(\"dc_solution\")\n",
    "\n",
    "# Paired summaries (medians)\n",
    "by_bm.to_csv(\"summary_by_base_model_medians.csv\", index=False)\n",
    "by_solution.to_csv(\"summary_by_dc_solution_medians.csv\", index=False)\n",
    "\n",
    "# Coverage of pairs (should be ~50 per variant×solution)\n",
    "coverage.to_csv(\"pair_coverage.csv\", index=False)\n",
    "\n",
    "# Optional: full paired rows with deltas\n",
    "pairs.to_csv(\"paired_rows_with_delta.csv\", index=False)\n",
    "\n",
    "# Optional nonparametric effect sizes\n",
    "cliff_bm.to_csv(\"cliffs_delta_by_base_model.csv\", index=False)\n",
    "cliff_sol.to_csv(\"cliffs_delta_by_dc_solution.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== Cliff's delta (CONTEXT vs ZEROSHOT) — BY BASE MODEL ===\")\n",
    "print(cliff_bm)\n",
    "print(\"\\n=== Cliff's delta (CONTEXT vs ZEROSHOT) — BY DC SOLUTION ===\")\n",
    "print(cliff_sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a9fd4",
   "metadata": {},
   "source": [
    "**EXPLANATIONS**\n",
    "1. ECDFs [Eucledian Cliff Delta Functions]: provides a clear view of the ratings and easy to spot ceiling ratings. \n",
    "    - if plotted (context vs. zeroshot) and the 'context curve' appearing lower means, more high ratings.\n",
    "    - easy to indetify whether the differences in ratings are global or not. \n",
    "2. Stacked proportions: \n",
    "    - proportions of composition of ratings per base model [4's, 3's etc], also which is much more than other\n",
    "3. Paired deltas (context - zeroshot)\n",
    "    - pair (same variant x solution x iteration)\n",
    "    - plots two conditions as side-by-side bars proportions and one after the other\n",
    "    - median delta: presents the shift (differences between medians of the two conditions)\n",
    "    - proportions improved/same/worse: between the two conditions\n",
    "    - pair_id = base_model x variant_id x dc_solution x iteration [two rows per pair, one from each condition]\n",
    "    - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
