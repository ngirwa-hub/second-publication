{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db50f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('unnumbered_list/barriers_biased_unnumbered_filled.csv')\n",
    "\n",
    "# barrier dictionary\n",
    "BARRIERS = {\n",
    "    1: \"power losses, quality and safety issues\",\n",
    "    2: \"reduced reliability in DC devices\",\n",
    "    3: \"lack of use-cases in which DC is advantageous\",\n",
    "    4: \"uncertain utility interaction (net metering, utility ownership, and agreed standards)\",\n",
    "    5: \"lack of pilot projects\",\n",
    "    6: \"public perception of DC and readiness to 'champion' installations from DC projects\",\n",
    "    7: \"incompatibility of DC systems components\",\n",
    "    8: \"misconception and lack of knowledge leads to lengthy/expensive design and permit process\",\n",
    "    9: \"lack of enough trained personnel in DC systems\",\n",
    "    10: \"uncertain regulatory roadmap\",\n",
    "    11: \"high costs of DC solutions\",\n",
    "}\n",
    "\n",
    "# Reverse the dictionary: barrier name -> id\n",
    "BARRIERS_REVERSED = {v: k for k, v in BARRIERS.items()}\n",
    "\n",
    "# Map 'official_label' to barrier id\n",
    "df['barrier_id'] = df['official_label'].map(BARRIERS_REVERSED)\n",
    "\n",
    "df.to_csv('unnumbered_list/barriers_biased_unnumbered_filled_labeled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5e77568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5018, 17)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"unnumbered_list/barriers_biased_unnumbered_filled_labeled.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b33c33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - unnumbered_list/duplicate_barriers_summary_unnumbered.csv\n",
      " - unnumbered_list/duplicate_rows_unnumbered.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"unnumbered_list/barriers_biased_unnumbered_filled_labeled.csv\")\n",
    "# ----- CONFIG -----\n",
    "EXPECTED_PER_ITER = 5\n",
    "OUT_ALL   = \"unnumbered_list/unnumbered_barrier_counts_by_model_variant_bias_iter.csv\"\n",
    "OUT_OVER  = \"unnumbered_list/unnumbered_barrier_counts_over_limit.csv\"\n",
    "\n",
    "# Normalize/typing\n",
    "df[\"base_model\"] = df[\"base_model\"].astype(str).str.strip()\n",
    "df[\"variant_id\"] = df[\"variant_id\"].astype(str).str.strip()\n",
    "df[\"bias_type\"]  = df.get(\"bias_type\", pd.Series(index=df.index)).astype(str).str.strip()\n",
    "df.loc[df[\"bias_type\"].eq(\"\"), \"bias_type\"] = np.nan  # treat empty as missing\n",
    "df[\"iteration\"]  = pd.to_numeric(df.get(\"iteration\", pd.Series(index=df.index)), errors=\"coerce\").astype(\"Int64\")\n",
    "df[\"barrier_id\"] = pd.to_numeric(df.get(\"barrier_id\", pd.Series(index=df.index)), errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "#take only rows with values in bias_type:\n",
    "df_anchor = df[df[\"bias_type\"].notna()].copy()\n",
    "\n",
    "# Group and count\n",
    "counts = (\n",
    "    df_anchor\n",
    "    .groupby([\"base_model\", \"variant_id\", \"bias_type\", \"iteration\"], dropna=False)\n",
    "    .agg(\n",
    "        rows=(\"row_id\", \"size\"),\n",
    "        n_unique_barriers=(\"barrier_id\", lambda s: s.dropna().nunique()),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        dupes=lambda d: d[\"rows\"] - d[\"n_unique_barriers\"],\n",
    "        over_rows=lambda d: d[\"rows\"] > EXPECTED_PER_ITER,\n",
    "        over_unique=lambda d: d[\"n_unique_barriers\"] > EXPECTED_PER_ITER,\n",
    "    )\n",
    "    .sort_values([\"base_model\", \"variant_id\", \"bias_type\", \"iteration\"])\n",
    ")\n",
    "\n",
    "# Save full counts and only-over-limit views\n",
    "counts.to_csv(OUT_ALL, index=False)\n",
    "counts.loc[counts[\"over_rows\"] | counts[\"over_unique\"]].to_csv(OUT_OVER, index=False)\n",
    "\n",
    "# ---------- DUPLICATE DETAILS (summaries + row-level) ----------\n",
    "OUT_DUPES_SUMMARY = \"unnumbered_list/duplicate_barriers_summary_unnumbered.csv\"\n",
    "OUT_DUPES_ROWS    = \"unnumbered_list/duplicate_rows_unnumbered.csv\"\n",
    "\n",
    "grp = [\"base_model\", \"variant_id\", \"bias_type\", \"iteration\", \"barrier_id\"]\n",
    "\n",
    "# Work only with rows that have a barrier_id (duplicates without a barrier_id don't make sense)\n",
    "df_anchor_nonnull = df_anchor.dropna(subset=[\"barrier_id\"]).copy()\n",
    "\n",
    "# Ensure we have a stable row identifier\n",
    "if \"row_id\" not in df_anchor_nonnull.columns:\n",
    "    df_anchor_nonnull[\"row_id\"] = df_anchor_nonnull.index.astype(int)\n",
    "\n",
    "# --- Summary per (combo + barrier_id): count + row IDs ---\n",
    "dupe_counts = (\n",
    "    df_anchor_nonnull\n",
    "      .groupby(grp, dropna=False)\n",
    "      .size()\n",
    "      .reset_index(name=\"rows_for_barrier\")\n",
    ")\n",
    "\n",
    "row_ids_agg = (\n",
    "    df_anchor_nonnull\n",
    "      .groupby(grp, dropna=False)[\"row_id\"]\n",
    "      .apply(list)\n",
    "      .reset_index(name=\"row_ids_for_barrier\")\n",
    ")\n",
    "\n",
    "dupe_summary = dupe_counts.merge(row_ids_agg, on=grp, how=\"left\")\n",
    "\n",
    "# Keep only actual duplicates (count > 1)\n",
    "dupe_summary_only = dupe_summary.loc[dupe_summary[\"rows_for_barrier\"] > 1].copy()\n",
    "\n",
    "# --- Row-level duplicate expansion ---\n",
    "# explode the list of row_ids to get per-row details\n",
    "dupe_rows = (\n",
    "    dupe_summary_only\n",
    "      .explode(\"row_ids_for_barrier\")\n",
    "      .rename(columns={\"row_ids_for_barrier\": \"row_id\"})\n",
    "      .merge(\n",
    "          df_anchor_nonnull,\n",
    "          on=[\"row_id\", \"base_model\", \"variant_id\", \"bias_type\", \"iteration\", \"barrier_id\"],\n",
    "          how=\"left\",\n",
    "          suffixes=(\"\", \"_orig\")\n",
    "      )\n",
    "      .sort_values([\"base_model\", \"variant_id\", \"bias_type\", \"iteration\", \"barrier_id\", \"row_id\"])\n",
    ")\n",
    "\n",
    "# Save files\n",
    "dupe_summary_only.to_csv(OUT_DUPES_SUMMARY, index=False)\n",
    "dupe_rows.to_csv(OUT_DUPES_ROWS, index=False)\n",
    "print(\" -\", OUT_DUPES_SUMMARY)\n",
    "print(\" -\", OUT_DUPES_ROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9733c2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5018, 17)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"unnumbered_list/dummy_cleaning/barriers_biased_unnumbered_filled_labeled.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51d1f6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Clean file saved: unnumbered_list/dummy_cleaning/barriers_clean.csv  (rows: 5007)\n",
      "✔ Duplicate rows saved: unnumbered_list/dummy_cleaning/barriers_duplicates.csv  (rows: 11)\n",
      "✔ Duplicate summary saved: unnumbered_list/dummy_cleaning/barriers_duplicates_summary.csv  (rows: 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_24948\\2949822638.py:33: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  ts = pd.to_datetime(anchor[\"timestamp\"], errors=\"coerce\")\n"
     ]
    }
   ],
   "source": [
    "#removing duplicates rows per combination of (base_model, variant_id, bias_type, iteration, barrier_id)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "INPUT_FILE   = \"unnumbered_list/dummy_cleaning/barriers_biased_unnumbered_filled_labeled.csv\"\n",
    "CLEAN_FILE   = \"unnumbered_list/dummy_cleaning/barriers_clean.csv\"\n",
    "DUPES_FILE   = \"unnumbered_list/dummy_cleaning/barriers_duplicates.csv\"\n",
    "SUMMARY_FILE = \"unnumbered_list/dummy_cleaning/barriers_duplicates_summary.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# --- Normalize/typing ---\n",
    "for col in [\"base_model\", \"variant_id\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "df[\"bias_type\"]  = df.get(\"bias_type\", pd.Series(index=df.index)).astype(str).str.strip()\n",
    "df.loc[df[\"bias_type\"].eq(\"\")] = np.nan\n",
    "df[\"iteration\"]  = pd.to_numeric(df.get(\"iteration\", pd.Series(index=df.index)), errors=\"coerce\").astype(\"Int64\")\n",
    "df[\"barrier_id\"] = pd.to_numeric(df.get(\"barrier_id\", pd.Series(index=df.index)), errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Ensure a stable row identifier (use existing if present, else create)\n",
    "if \"row_id\" not in df.columns:\n",
    "    df[\"row_id\"] = df.index.astype(int)\n",
    "\n",
    "# Work only on anchor (biased) rows; keep others as-is\n",
    "anchor_mask = df[\"bias_type\"].notna()\n",
    "anchor = df.loc[anchor_mask].copy()\n",
    "non_anchor = df.loc[~anchor_mask].copy()\n",
    "\n",
    "# Sort so \"first\" is meaningful; prefer earliest timestamp if available\n",
    "if \"timestamp\" in anchor.columns:\n",
    "    # If timestamp is parseable, use it; otherwise falls back to current order\n",
    "    ts = pd.to_datetime(anchor[\"timestamp\"], errors=\"coerce\")\n",
    "    anchor = anchor.assign(_ts=ts).sort_values([\"base_model\",\"variant_id\",\"bias_type\",\"iteration\",\"barrier_id\",\"_ts\",\"row_id\"])\n",
    "else:\n",
    "    anchor = anchor.sort_values([\"base_model\",\"variant_id\",\"bias_type\",\"iteration\",\"barrier_id\",\"row_id\"])\n",
    "\n",
    "KEY = [\"base_model\",\"variant_id\",\"bias_type\",\"iteration\",\"barrier_id\"]\n",
    "\n",
    "# Extra duplicates beyond the first occurrence in each KEY\n",
    "dupes = anchor[anchor.duplicated(subset=KEY, keep=\"first\")].copy()\n",
    "\n",
    "# Cleaned anchor = keep first per KEY\n",
    "anchor_clean = anchor.drop_duplicates(subset=KEY, keep=\"first\").copy()\n",
    "\n",
    "# Recombine with non-anchor rows\n",
    "clean = pd.concat([anchor_clean.drop(columns=[c for c in [\"_ts\"] if c in anchor_clean.columns]),\n",
    "                   non_anchor], ignore_index=True)\n",
    "\n",
    "# Optional: sort final output for readability\n",
    "clean = clean.sort_values([\"base_model\",\"variant_id\",\"bias_type\",\"iteration\",\"barrier_id\",\"row_id\"], na_position=\"last\")\n",
    "\n",
    "# --- Save outputs ---\n",
    "clean.to_csv(CLEAN_FILE, index=False)\n",
    "dupes.to_csv(DUPES_FILE, index=False)\n",
    "\n",
    "# Summary by combo + barrier (how many extras removed)\n",
    "if not dupes.empty:\n",
    "    summary = (dupes\n",
    "               .groupby(KEY, dropna=False)\n",
    "               .agg(n_extra_dupes=(\"row_id\",\"size\"),\n",
    "                    row_ids=(\"row_id\", lambda s: list(s)))\n",
    "               .reset_index()\n",
    "               .sort_values(KEY))\n",
    "    summary.to_csv(SUMMARY_FILE, index=False)\n",
    "\n",
    "print(f\"✔ Clean file saved: {CLEAN_FILE}  (rows: {len(clean)})\")\n",
    "print(f\"✔ Duplicate rows saved: {DUPES_FILE}  (rows: {len(dupes)})\")\n",
    "if 'summary' in locals():\n",
    "    print(f\"✔ Duplicate summary saved: {SUMMARY_FILE}  (rows: {len(summary)})\")\n",
    "else:\n",
    "    print(\"✔ No within-run duplicates found according to KEY.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d78ff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Clean file saved to: unnumbered_list/unnumbered_barriers_max5_clean.csv\n",
      "✔ Extra rows saved to: unnumbered_list/dummy_cleaning/unnumbered_barriers_extras.csv\n"
     ]
    }
   ],
   "source": [
    "#eliminate extra exceeding 5\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Input and output paths\n",
    "INPUT_FILE = \"unnumbered_list/dummy_cleaning/barriers_clean.csv\"\n",
    "CLEAN_FILE = \"unnumbered_list/unnumbered_barriers_max5_clean.csv\"\n",
    "EXTRA_FILE = \"unnumbered_list/dummy_cleaning/unnumbered_barriers_extras.csv\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# Define grouping columns (adjust as needed)\n",
    "GROUP_COLS = [\"base_model\", \"variant_id\", \"bias_type\", \"iteration\"]\n",
    "\n",
    "# Rank rows within each group\n",
    "df[\"_rank\"] = df.groupby(GROUP_COLS).cumcount() + 1\n",
    "\n",
    "# Split into kept (≤5) and eliminated (>5)\n",
    "clean = df[df[\"_rank\"] <= 5].drop(columns=\"_rank\")\n",
    "extras = df[df[\"_rank\"] > 5].drop(columns=\"_rank\")\n",
    "\n",
    "# Save to CSV\n",
    "clean.to_csv(CLEAN_FILE, index=False)\n",
    "extras.to_csv(EXTRA_FILE, index=False)\n",
    "\n",
    "print(\"✔ Clean file saved to:\", CLEAN_FILE)\n",
    "print(\"✔ Extra rows saved to:\", EXTRA_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db20738c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4791, 17)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"unnumbered_list/unnumbered_barriers_max5_clean.csv\")\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cea89a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote barriers_dedup.csv\n",
      "✅ Wrote barrier_counts_after_dedup.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "base_model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "variant_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "bias_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "iteration",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rows",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n_unique_barriers",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dupes_removed",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "b1aa250c-d9e6-42a7-a41f-cd69a72b636e",
       "rows": [
        [
         "0",
         "gemma3",
         "gemma3_generalist",
         "BIAS_EXAMPLE",
         "0",
         "5",
         "5",
         "0"
        ],
        [
         "1",
         "gemma3",
         "gemma3_generalist",
         "BIAS_EXAMPLE",
         "1",
         "5",
         "5",
         "0"
        ],
        [
         "2",
         "gemma3",
         "gemma3_generalist",
         "BIAS_EXAMPLE",
         "2",
         "5",
         "5",
         "0"
        ],
        [
         "3",
         "gemma3",
         "gemma3_generalist",
         "BIAS_EXAMPLE",
         "3",
         "5",
         "5",
         "0"
        ],
        [
         "4",
         "gemma3",
         "gemma3_generalist",
         "BIAS_EXAMPLE",
         "4",
         "5",
         "5",
         "0"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_model</th>\n",
       "      <th>variant_id</th>\n",
       "      <th>bias_type</th>\n",
       "      <th>iteration</th>\n",
       "      <th>rows</th>\n",
       "      <th>n_unique_barriers</th>\n",
       "      <th>dupes_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma3</td>\n",
       "      <td>gemma3_generalist</td>\n",
       "      <td>BIAS_EXAMPLE</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemma3</td>\n",
       "      <td>gemma3_generalist</td>\n",
       "      <td>BIAS_EXAMPLE</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemma3</td>\n",
       "      <td>gemma3_generalist</td>\n",
       "      <td>BIAS_EXAMPLE</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemma3</td>\n",
       "      <td>gemma3_generalist</td>\n",
       "      <td>BIAS_EXAMPLE</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gemma3</td>\n",
       "      <td>gemma3_generalist</td>\n",
       "      <td>BIAS_EXAMPLE</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  base_model         variant_id     bias_type  iteration  rows  \\\n",
       "0     gemma3  gemma3_generalist  BIAS_EXAMPLE          0     5   \n",
       "1     gemma3  gemma3_generalist  BIAS_EXAMPLE          1     5   \n",
       "2     gemma3  gemma3_generalist  BIAS_EXAMPLE          2     5   \n",
       "3     gemma3  gemma3_generalist  BIAS_EXAMPLE          3     5   \n",
       "4     gemma3  gemma3_generalist  BIAS_EXAMPLE          4     5   \n",
       "\n",
       "   n_unique_barriers  dupes_removed  \n",
       "0                  5              0  \n",
       "1                  5              0  \n",
       "2                  5              0  \n",
       "3                  5              0  \n",
       "4                  5              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"barriers_biased_inverse_0253hrs_labeled.csv\")\n",
    "# --- Key columns / typing ---\n",
    "COMBO = [\"base_model\", \"variant_id\", \"bias_type\", \"iteration\"]  # adjust if needed\n",
    "# Optional: normalize strings\n",
    "for c in [\"base_model\", \"variant_id\", \"bias_type\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# --- Choose sort priority so we know which duplicate to keep ---\n",
    "# If you have a 'list_order' or 'timestamp' column, sort by those first.\n",
    "sort_cols = []\n",
    "if \"list_order\" in df.columns:\n",
    "    sort_cols.append(\"list_order\")\n",
    "if \"timestamp\" in df.columns:\n",
    "    sort_cols.append(\"timestamp\")\n",
    "# always end with row_id for stability if present\n",
    "if \"row_id\" in df.columns:\n",
    "    sort_cols.append(\"row_id\")\n",
    "\n",
    "# Build full sort order: combo keys + chosen sort columns\n",
    "sort_by = COMBO + sort_cols if sort_cols else COMBO\n",
    "df_sorted = df.sort_values(sort_by)\n",
    "\n",
    "# --- Remove duplicates: keep first barrier_id per combo ---\n",
    "dedup = df_sorted.drop_duplicates(subset=COMBO + [\"barrier_id\"], keep=\"first\")\n",
    "\n",
    "# --- Save deduped rows ---\n",
    "dedup.to_csv(\"barriers_dedup.csv\", index=False)\n",
    "print(\"✅ Wrote barriers_dedup.csv\")\n",
    "\n",
    "# --- (Optional) counts after dedup, to inspect remaining sizes ---\n",
    "after_counts = (\n",
    "    dedup.groupby(COMBO, dropna=False)\n",
    "         .agg(\n",
    "             rows=(\"row_id\", \"size\"),\n",
    "             n_unique_barriers=(\"barrier_id\", lambda s: s.dropna().nunique()),\n",
    "         )\n",
    "         .reset_index()\n",
    "         .assign(dupes_removed=lambda d: d[\"rows\"] - d[\"n_unique_barriers\"])\n",
    "         .sort_values(COMBO)\n",
    ")\n",
    "after_counts.to_csv(\"barrier_counts_after_dedup.csv\", index=False)\n",
    "print(\"✅ Wrote barrier_counts_after_dedup.csv\")\n",
    "\n",
    "# Quick peek\n",
    "display(after_counts.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58d790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote barriers_dedup_first5_preserve_order.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"barriers_biased_inverse_0253hrs_labeled.csv\")\n",
    "\n",
    "# Define the per-run combination (edit if needed)\n",
    "COMBO = [\"base_model\", \"variant_id\", \"bias_type\", \"iteration\"]\n",
    "IDCOL = \"barrier_id\"\n",
    "\n",
    "# Keep a stable original-row order key\n",
    "df[\"_ord\"] = np.arange(len(df))\n",
    "\n",
    "# Ensure types (so drop_duplicates works reliably)\n",
    "df[IDCOL] = pd.to_numeric(df[IDCOL], errors=\"coerce\").astype(\"Int64\")\n",
    "df[\"iteration\"] = pd.to_numeric(df.get(\"iteration\", pd.Series(index=df.index)), errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# 1) Remove duplicate barrier_id within each combo, keeping the first occurrence in the ORIGINAL order\n",
    "dedup = df.drop_duplicates(subset=COMBO + [IDCOL], keep=\"first\")\n",
    "\n",
    "# 2) Keep only the first 5 rows per combo (again, by ORIGINAL order)\n",
    "dedup = (\n",
    "    dedup.sort_values(\"_ord\")              # ensure original order\n",
    "         .groupby(COMBO, sort=False)\n",
    "         .head(5)                          # keep at most 5 per combo\n",
    "         .sort_values(\"_ord\")              # restore overall original order\n",
    "         .drop(columns=[\"_ord\"])           # cleanup\n",
    ")\n",
    "\n",
    "# Save the clean CSV\n",
    "dedup.to_csv(\"barriers_dedup_first5_preserve_order.csv\", index=False)\n",
    "print(\"✅ Wrote barriers_dedup_first5_preserve_order.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56f4268b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote:\n",
      " - reversedlist_selection_counts_by_combo_wide.csv\n",
      " - reversedlist_selection_rates_by_combo_wide.csv\n",
      " - reversedlist_jaccard_vs_canonical_first5_by_combo.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df=pd.read_csv(\"reversedbarrier_anchor_okay.csv\")\n",
    "\n",
    "# barrier dictionary\n",
    "BARRIERS = {\n",
    "    1: \"high costs of DC solutions\",\n",
    "    2: \"uncertain regulatory roadmap\",\n",
    "    3: \"lack of enough trained personnel in DC systems\",\n",
    "    4: \"misconception and lack of knowledge leads to lengthy/expensive design and permit process\",\n",
    "    5: \"incompatibility of DC systems components\",\n",
    "    6: \"public perception of DC and readiness to 'champion' installations from DC projects\",\n",
    "    7: \"lack of pilot projects\",\n",
    "    8: \"uncertain utility interaction (net metering, utility ownership, and agreed standards)\",\n",
    "    9: \"lack of use-cases in which DC is advantageous\",\n",
    "    10: \"reduced reliability in DC devices\",\n",
    "    11: \"power losses, quality and safety issues\"\n",
    "}\n",
    "\n",
    "CANON_FIRST5 = set(list(BARRIERS.keys())[:5])\n",
    "\n",
    "# ---------------- Assumes df is already in memory ----------------\n",
    "# Required columns: base_model, variant_id, model, barrier_id\n",
    "# Optional but harmless: bias_type, iteration, timestamp, etc.\n",
    "\n",
    "# Light typing / cleaning\n",
    "df = df.copy()\n",
    "df[\"base_model\"] = df[\"base_model\"].astype(str).str.strip()\n",
    "df[\"variant_id\"] = df[\"variant_id\"].astype(str).str.strip()\n",
    "df[\"model\"]      = df[\"model\"].astype(str).str.strip()\n",
    "df[\"barrier_id\"] = pd.to_numeric(df[\"barrier_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# ---------------- 1) Selection counts & rates per combo ----------------\n",
    "GROUP = [\"base_model\", \"variant_id\", \"model\"]\n",
    "\n",
    "counts = (\n",
    "    df.dropna(subset=[\"barrier_id\"])\n",
    "      .groupby(GROUP + [\"barrier_id\"], dropna=False)\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "totals = counts.groupby(GROUP, dropna=False)[\"count\"].sum().reset_index(name=\"total_count\")\n",
    "\n",
    "counts = counts.merge(totals, on=GROUP, how=\"left\")\n",
    "counts[\"rate\"] = counts[\"count\"] / counts[\"total_count\"].replace(0, np.nan)\n",
    "\n",
    "# Wide tables: one column per barrier_id (counts and rates)\n",
    "counts_wide = (\n",
    "    counts.pivot_table(index=GROUP, columns=\"barrier_id\", values=\"count\", fill_value=0)\n",
    "          .rename_axis(None, axis=1)\n",
    "          .reset_index()\n",
    ")\n",
    "rates_wide = (\n",
    "    counts.pivot_table(index=GROUP, columns=\"barrier_id\", values=\"rate\", fill_value=0.0)\n",
    "          .rename_axis(None, axis=1)\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "# Add total_count to both for convenience\n",
    "counts_wide = counts_wide.merge(totals, on=GROUP, how=\"left\")\n",
    "rates_wide  = rates_wide.merge(totals, on=GROUP, how=\"left\")\n",
    "\n",
    "# Save selection tables\n",
    "counts_wide.to_csv(\"reversedlist_selection_counts_by_combo_wide.csv\", index=False)\n",
    "rates_wide.to_csv(\"reversedlist_selection_rates_by_combo_wide.csv\", index=False)\n",
    "\n",
    "# --- canonical first 5 from dict insertion order ---\n",
    "CANON_FIRST5_LIST = list(BARRIERS.keys())[:5]\n",
    "CANON_FIRST5 = set(CANON_FIRST5_LIST)\n",
    "\n",
    "def jaccard(a: set, b: set) -> float:\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    return len(a & b) / len(a | b) if (a or b) else 0.0\n",
    "\n",
    "# --- build top-5 per GROUP: both set and ordered list ---\n",
    "# sort by GROUP (asc) then count (desc)\n",
    "sorted_counts = counts.sort_values(\n",
    "    GROUP + [\"count\"],\n",
    "    ascending=[True]*len(GROUP) + [False]\n",
    ")\n",
    "\n",
    "# take top-5 rows per combo\n",
    "top5_rows = (\n",
    "    sorted_counts\n",
    "    .groupby(GROUP, dropna=False)\n",
    "    .head(5)\n",
    ")\n",
    "\n",
    "# ordered list (by count desc) and set for Jaccard\n",
    "top5_ordered = (\n",
    "    top5_rows.groupby(GROUP, dropna=False)[\"barrier_id\"]\n",
    "             .apply(lambda s: [int(x) for x in s.tolist()])           # keep order\n",
    "             .reset_index(name=\"top5_selected\")                        # ordered list\n",
    ")\n",
    "\n",
    "top5_sets = (\n",
    "    top5_rows.groupby(GROUP, dropna=False)[\"barrier_id\"]\n",
    "             .apply(lambda s: set(int(x) for x in s.dropna().tolist()))\n",
    "             .reset_index(name=\"top5_set\")                             # set for Jaccard\n",
    ")\n",
    "\n",
    "top5 = top5_ordered.merge(top5_sets, on=GROUP, how=\"left\")\n",
    "top5[\"jaccard_top5_vs_first5\"] = top5[\"top5_set\"].map(lambda S: jaccard(S, CANON_FIRST5))\n",
    "\n",
    "# (optional) unique-set section can stay or be removed; keeping here for context\n",
    "unique_sets = (\n",
    "    df.dropna(subset=[\"barrier_id\"])\n",
    "      .groupby(GROUP, dropna=False)[\"barrier_id\"]\n",
    "      .apply(lambda s: set(int(x) for x in s.dropna().tolist()))\n",
    "      .reset_index(name=\"selected_set\")\n",
    ")\n",
    "unique_sets[\"n_selected_unique\"] = unique_sets[\"selected_set\"].map(len)\n",
    "\n",
    "# --- assemble output with easy-to-audit columns ---\n",
    "jaccard_out = (\n",
    "    totals\n",
    "    .merge(unique_sets, on=GROUP, how=\"left\")\n",
    "    .merge(top5,        on=GROUP, how=\"left\")\n",
    "    .assign(\n",
    "        # readable versions\n",
    "        selected_set=lambda d: d[\"selected_set\"].map(lambda s: sorted(list(s)) if isinstance(s, set) else []),\n",
    "        canon_first5=lambda d: [CANON_FIRST5_LIST]*len(d)\n",
    "    )[\n",
    "        GROUP\n",
    "        + [\"total_count\",\n",
    "           \"n_selected_unique\", \"selected_set\",\n",
    "           \"top5_selected\", \"jaccard_top5_vs_first5\", \"canon_first5\"]\n",
    "    ]\n",
    "    .sort_values(GROUP)\n",
    ")\n",
    "\n",
    "jaccard_out.to_csv(\"jaccard_vs_canonical_first5_by_combo.csv\", index=False)\n",
    "\n",
    "\n",
    "print(\"✅ Wrote:\")\n",
    "print(\" - reversedlist_selection_counts_by_combo_wide.csv\")\n",
    "print(\" - reversedlist_selection_rates_by_combo_wide.csv\")\n",
    "print(\" - reversedlist_jaccard_vs_canonical_first5_by_combo.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5aaf875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - reversed_list/reversedlist_jaccard_compressed.csv\n",
      " - reversed_list/reversedlist_selection_counts_compressed.csv\n",
      " - reversed_list/reversedlist_selection_rates_compressed.csv\n"
     ]
    }
   ],
   "source": [
    "#COMPRESSED FILES REVERSED BARRIER LIST\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"reversed_list/reversedbarrier_anchor_okay.csv\")\n",
    "\n",
    "# ---------------- Canonical barrier list (source of truth) ----------------\n",
    "# reversed- barrier dictionary\n",
    "BARRIERS = {\n",
    "    1: \"high costs of DC solutions\",\n",
    "    2: \"uncertain regulatory roadmap\",\n",
    "    3: \"lack of enough trained personnel in DC systems\",\n",
    "    4: \"misconception and lack of knowledge leads to lengthy/expensive design and permit process\",\n",
    "    5: \"incompatibility of DC systems components\",\n",
    "    6: \"public perception of DC and readiness to 'champion' installations from DC projects\",\n",
    "    7: \"lack of pilot projects\",\n",
    "    8: \"uncertain utility interaction (net metering, utility ownership, and agreed standards)\",\n",
    "    9: \"lack of use-cases in which DC is advantageous\",\n",
    "    10: \"reduced reliability in DC devices\",\n",
    "    11: \"power losses, quality and safety issues\"\n",
    "}\n",
    "\n",
    "CANON_FIRST5 = set(list(BARRIERS.keys())[:5])\n",
    "\n",
    "# ---------------- Assumes df is already in memory ----------------\n",
    "# Required columns: base_model, variant_id, model, barrier_id\n",
    "# Optional but harmless: bias_type, iteration, timestamp, etc.\n",
    "\n",
    "# Light typing / cleaning\n",
    "df = df.copy()\n",
    "df[\"base_model\"] = df[\"base_model\"].astype(str).str.strip()\n",
    "df[\"variant_id\"] = df[\"variant_id\"].astype(str).str.strip()\n",
    "df[\"model\"]      = df[\"model\"].astype(str).str.strip()\n",
    "df[\"barrier_id\"] = pd.to_numeric(df[\"barrier_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# ---------------- 1) Selection counts & rates per combo ----------------\n",
    "GROUP = [\"base_model\", \"variant_id\", \"model\"]\n",
    "GROUP_BASE =[\"base_model\"]\n",
    "\n",
    "\n",
    "# --- canonical first 5 from dict insertion order ---\n",
    "CANON_FIRST5_LIST = list(BARRIERS.keys())[:5]\n",
    "CANON_FIRST5 = set(CANON_FIRST5_LIST)\n",
    "\n",
    "def jaccard(a: set, b: set) -> float:\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    return len(a & b) / len(a | b) if (a or b) else 0.0\n",
    "\n",
    "counts_base = (\n",
    "    df.dropna(subset=[\"barrier_id\"])\n",
    "      .groupby(GROUP_BASE + [\"barrier_id\"], dropna=False)\n",
    "      .size()\n",
    "        .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "totals_base = (\n",
    "    counts_base.groupby(GROUP_BASE, dropna=False)[\"count\"]\n",
    "            .sum()\n",
    "            .reset_index(name=\"total_count\")\n",
    ")\n",
    "\n",
    "#top-5 by aggregated counts\n",
    "sorted_counts_base = counts_base.sort_values(\n",
    "    GROUP_BASE + [\"count\", \"barrier_id\"],\n",
    "    ascending=[True]*len(GROUP_BASE) + [False, True]\n",
    ")\n",
    "\n",
    "top5_rows_base=(\n",
    "    sorted_counts_base\n",
    "    .groupby(GROUP_BASE, dropna=False)\n",
    "    .head(5)\n",
    ")\n",
    "\n",
    "top5_ordered_base = (\n",
    "    top5_rows_base.groupby(GROUP_BASE, dropna=False)[\"barrier_id\"]\n",
    "                  .apply(lambda s: [int(x) for x in s.tolist()])   # keep order\n",
    "                  .reset_index(name=\"top5_selected\")\n",
    ")\n",
    "\n",
    "# Union of all selected barriers across variants for each base\n",
    "unique_sets_base = (\n",
    "    df.dropna(subset=[\"barrier_id\"])\n",
    "      .groupby(GROUP_BASE, dropna=False)[\"barrier_id\"]\n",
    "      .apply(lambda s: set(int(x) for x in s.dropna().tolist()))\n",
    "      .reset_index(name=\"selected_set\")\n",
    ")\n",
    "unique_sets_base[\"n_selected_unique\"] = unique_sets_base[\"selected_set\"].map(len)\n",
    "\n",
    "# Assemble base-level output (same columns schema)\n",
    "jaccard_out_base = (\n",
    "    totals_base\n",
    "    .merge(unique_sets_base, on=GROUP_BASE, how=\"left\")\n",
    "    .merge(top5_ordered_base, on=GROUP_BASE, how=\"left\")\n",
    "    .assign(\n",
    "        jaccard_top5_vs_first5=lambda d: d[\"top5_selected\"].map(lambda lst: jaccard(set(lst or []), set(CANON_FIRST5_LIST))),\n",
    "        # readable versions\n",
    "        selected_set=lambda d: d[\"selected_set\"].map(lambda s: sorted(list(s)) if isinstance(s, set) else []),\n",
    "        canon_first5=[CANON_FIRST5_LIST]*len(totals_base)\n",
    "    )[\n",
    "        [\"base_model\",\n",
    "         \"total_count\",\n",
    "         \"n_selected_unique\", \"selected_set\",\n",
    "         \"top5_selected\", \"jaccard_top5_vs_first5\", \"canon_first5\"]\n",
    "    ]\n",
    "    .sort_values(\"base_model\")\n",
    ")\n",
    "\n",
    "jaccard_out_base.to_csv(\"reversed_list/reversedlist_jaccard_compressed.csv\", index=False)\n",
    "\n",
    "print(\" - reversed_list/reversedlist_jaccard_compressed.csv\")\n",
    "\n",
    "# All barrier ids (to fix a consistent column order)\n",
    "barrier_ids_sorted = sorted(int(x) for x in df[\"barrier_id\"].dropna().unique())\n",
    "\n",
    "# Aggregated counts per base (combine all variants)\n",
    "counts_base = (\n",
    "    df.dropna(subset=[\"barrier_id\"])\n",
    "      .groupby(GROUP_BASE + [\"barrier_id\"], dropna=False)\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# Total selections per base\n",
    "totals_base = (\n",
    "    counts_base.groupby(GROUP_BASE, dropna=False)[\"count\"]\n",
    "               .sum()\n",
    "               .reset_index(name=\"total_count\")\n",
    ")\n",
    "\n",
    "# ---- COUNTS wide (one row per base, one column per barrier) ----\n",
    "counts_base_wide = (\n",
    "    counts_base.pivot_table(index=GROUP_BASE, columns=\"barrier_id\", values=\"count\", fill_value=0)\n",
    "               .rename_axis(None, axis=1)\n",
    "               .reset_index()\n",
    ")\n",
    "\n",
    "# Ensure barrier columns appear in numeric order B1..B* (as raw ints, same as your original)\n",
    "ordered_cols_counts = [\"base_model\"] + barrier_ids_sorted\n",
    "# Add missing columns (if any barriers absent for all bases)\n",
    "for c in barrier_ids_sorted:\n",
    "    if c not in counts_base_wide.columns:\n",
    "        counts_base_wide[c] = 0\n",
    "counts_base_wide = counts_base_wide.reindex(columns=ordered_cols_counts)\n",
    "\n",
    "# Append total_count\n",
    "counts_base_wide = counts_base_wide.merge(totals_base, on=\"base_model\", how=\"left\")\n",
    "\n",
    "# ---- RATES wide (counts / total_count per base) ----\n",
    "rate_base = counts_base.merge(totals_base, on=\"base_model\", how=\"left\")\n",
    "rate_base[\"rate\"] = rate_base[\"count\"] / rate_base[\"total_count\"].replace(0, np.nan)\n",
    "\n",
    "rates_base_wide = (\n",
    "    rate_base.pivot_table(index=GROUP_BASE, columns=\"barrier_id\", values=\"rate\", fill_value=0.0)\n",
    "             .rename_axis(None, axis=1)\n",
    "             .reset_index()\n",
    ")\n",
    "\n",
    "# Ensure same barrier column order\n",
    "ordered_cols_rates = [\"base_model\"] + barrier_ids_sorted\n",
    "for c in barrier_ids_sorted:\n",
    "    if c not in rates_base_wide.columns:\n",
    "        rates_base_wide[c] = 0.0\n",
    "rates_base_wide = rates_base_wide.reindex(columns=ordered_cols_rates)\n",
    "\n",
    "# Append total_count for convenience/verification\n",
    "rates_base_wide = rates_base_wide.merge(totals_base, on=\"base_model\", how=\"left\")\n",
    "\n",
    "# ---- Write CSVs ----\n",
    "counts_base_wide.to_csv(\"reversed_list/reversedlist_selection_counts_compressed.csv\", index=False)\n",
    "rates_base_wide.to_csv(\"reversed_list/reversedlist_selection_rates_compressed.csv\", index=False)\n",
    "\n",
    "print(\" - reversed_list/reversedlist_selection_counts_compressed.csv\")\n",
    "print(\" - reversed_list/reversedlist_selection_rates_compressed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25f3a3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote:\n",
      " - orderedlist_selection_counts_by_combo_wide.csv\n",
      " - orderedlist_selection_rates_by_combo_wide.csv\n",
      " - orderedlist_jaccard_vs_canonical_first5_by_combo.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"orderedbarrier_anchor_okay.csv\")\n",
    "\n",
    "# ---------------- Canonical barrier list (source of truth) ----------------\n",
    "BARRIERS = {\n",
    "    1: \"power losses, quality and safety issues\",\n",
    "    2: \"reduced reliability in DC devices\",\n",
    "    3: \"lack of use-cases in which DC is advantageous\",\n",
    "    4: \"uncertain utility interaction (net metering, utility ownership, and agreed standards)\",\n",
    "    5: \"lack of pilot projects\",\n",
    "    6: \"public perception of DC and readiness to 'champion' installations from DC projects\",\n",
    "    7: \"incompatibility of DC systems components\",\n",
    "    8: \"misconception and lack of knowledge leads to lengthy/expensive design and permit process\",\n",
    "    9: \"lack of enough trained personnel in DC systems\",\n",
    "    10: \"uncertain regulatory roadmap\",\n",
    "    11: \"high costs of DC solutions\",\n",
    "}\n",
    "\n",
    "CANON_FIRST5 = set(list(BARRIERS.keys())[:5])\n",
    "\n",
    "# ---------------- Assumes df is already in memory ----------------\n",
    "# Required columns: base_model, variant_id, model, barrier_id\n",
    "# Optional but harmless: bias_type, iteration, timestamp, etc.\n",
    "\n",
    "# Light typing / cleaning\n",
    "df = df.copy()\n",
    "df[\"base_model\"] = df[\"base_model\"].astype(str).str.strip()\n",
    "df[\"variant_id\"] = df[\"variant_id\"].astype(str).str.strip()\n",
    "df[\"model\"]      = df[\"model\"].astype(str).str.strip()\n",
    "df[\"barrier_id\"] = pd.to_numeric(df[\"barrier_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# ---------------- 1) Selection counts & rates per combo ----------------\n",
    "GROUP = [\"base_model\", \"variant_id\", \"model\"]\n",
    "\n",
    "counts = (\n",
    "    df.dropna(subset=[\"barrier_id\"])\n",
    "      .groupby(GROUP + [\"barrier_id\"], dropna=False)\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "totals = counts.groupby(GROUP, dropna=False)[\"count\"].sum().reset_index(name=\"total_count\")\n",
    "\n",
    "counts = counts.merge(totals, on=GROUP, how=\"left\")\n",
    "counts[\"rate\"] = counts[\"count\"] / counts[\"total_count\"].replace(0, np.nan)\n",
    "\n",
    "# Wide tables: one column per barrier_id (counts and rates)\n",
    "counts_wide = (\n",
    "    counts.pivot_table(index=GROUP, columns=\"barrier_id\", values=\"count\", fill_value=0)\n",
    "          .rename_axis(None, axis=1)\n",
    "          .reset_index()\n",
    ")\n",
    "rates_wide = (\n",
    "    counts.pivot_table(index=GROUP, columns=\"barrier_id\", values=\"rate\", fill_value=0.0)\n",
    "          .rename_axis(None, axis=1)\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "# Add total_count to both for convenience\n",
    "counts_wide = counts_wide.merge(totals, on=GROUP, how=\"left\")\n",
    "rates_wide  = rates_wide.merge(totals, on=GROUP, how=\"left\")\n",
    "\n",
    "# Save selection tables\n",
    "counts_wide.to_csv(\"orderedlist_selection_counts_by_combo_wide.csv\", index=False)\n",
    "rates_wide.to_csv(\"orderedlist_selection_rates_by_combo_wide.csv\", index=False)\n",
    "\n",
    "# --- canonical first 5 from dict insertion order ---\n",
    "CANON_FIRST5_LIST = list(BARRIERS.keys())[:5]\n",
    "CANON_FIRST5 = set(CANON_FIRST5_LIST)\n",
    "\n",
    "def jaccard(a: set, b: set) -> float:\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    return len(a & b) / len(a | b) if (a or b) else 0.0\n",
    "\n",
    "# --- build top-5 per GROUP: both set and ordered list ---\n",
    "# sort by GROUP (asc) then count (desc)\n",
    "sorted_counts = counts.sort_values(\n",
    "    GROUP + [\"count\"],\n",
    "    ascending=[True]*len(GROUP) + [False]\n",
    ")\n",
    "\n",
    "# take top-5 rows per combo\n",
    "top5_rows = (\n",
    "    sorted_counts\n",
    "    .groupby(GROUP, dropna=False)\n",
    "    .head(5)\n",
    ")\n",
    "\n",
    "# ordered list (by count desc) and set for Jaccard\n",
    "top5_ordered = (\n",
    "    top5_rows.groupby(GROUP, dropna=False)[\"barrier_id\"]\n",
    "             .apply(lambda s: [int(x) for x in s.tolist()])           # keep order\n",
    "             .reset_index(name=\"top5_selected\")                        # ordered list\n",
    ")\n",
    "\n",
    "top5_sets = (\n",
    "    top5_rows.groupby(GROUP, dropna=False)[\"barrier_id\"]\n",
    "             .apply(lambda s: set(int(x) for x in s.dropna().tolist()))\n",
    "             .reset_index(name=\"top5_set\")                             # set for Jaccard\n",
    ")\n",
    "\n",
    "top5 = top5_ordered.merge(top5_sets, on=GROUP, how=\"left\")\n",
    "top5[\"jaccard_top5_vs_first5\"] = top5[\"top5_set\"].map(lambda S: jaccard(S, CANON_FIRST5))\n",
    "\n",
    "# (optional) unique-set section can stay or be removed; keeping here for context\n",
    "unique_sets = (\n",
    "    df.dropna(subset=[\"barrier_id\"])\n",
    "      .groupby(GROUP, dropna=False)[\"barrier_id\"]\n",
    "      .apply(lambda s: set(int(x) for x in s.dropna().tolist()))\n",
    "      .reset_index(name=\"selected_set\")\n",
    ")\n",
    "unique_sets[\"n_selected_unique\"] = unique_sets[\"selected_set\"].map(len)\n",
    "\n",
    "# --- assemble output with easy-to-audit columns ---\n",
    "jaccard_out = (\n",
    "    totals\n",
    "    .merge(unique_sets, on=GROUP, how=\"left\")\n",
    "    .merge(top5,        on=GROUP, how=\"left\")\n",
    "    .assign(\n",
    "        # readable versions\n",
    "        selected_set=lambda d: d[\"selected_set\"].map(lambda s: sorted(list(s)) if isinstance(s, set) else []),\n",
    "        canon_first5=lambda d: [CANON_FIRST5_LIST]*len(d)\n",
    "    )[\n",
    "        GROUP\n",
    "        + [\"total_count\",\n",
    "           \"n_selected_unique\", \"selected_set\",\n",
    "           \"top5_selected\", \"jaccard_top5_vs_first5\", \"canon_first5\"]\n",
    "    ]\n",
    "    .sort_values(GROUP)\n",
    ")\n",
    "\n",
    "jaccard_out.to_csv(\"orderedlist_jaccard_vs_canonical_first5_by_combo.csv\", index=False)\n",
    "\n",
    "\n",
    "print(\"✅ Wrote:\")\n",
    "print(\" - orderedlist_selection_counts_by_combo_wide.csv\")\n",
    "print(\" - orderedlist_selection_rates_by_combo_wide.csv\")\n",
    "print(\" - orderedlist_jaccard_vs_canonical_first5_by_combo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f96a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - ordered_list/orderedlist_jaccard_compressed.csv\n",
      " - ordered_list/orderedlist_selection_counts_compressed.csv\n",
      " - ordered_list/orderedlist_selection_rates_compressed.csv\n"
     ]
    }
   ],
   "source": [
    "# compressed files - ordered barrier list\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"ordered_list/orderedbarrier_anchor_okay.csv\")\n",
    "\n",
    "# ---------------- Canonical barrier list (source of truth) ----------------\n",
    "BARRIERS = {\n",
    "    1: \"power losses, quality and safety issues\",\n",
    "    2: \"reduced reliability in DC devices\",\n",
    "    3: \"lack of use-cases in which DC is advantageous\",\n",
    "    4: \"uncertain utility interaction (net metering, utility ownership, and agreed standards)\",\n",
    "    5: \"lack of pilot projects\",\n",
    "    6: \"public perception of DC and readiness to 'champion' installations from DC projects\",\n",
    "    7: \"incompatibility of DC systems components\",\n",
    "    8: \"misconception and lack of knowledge leads to lengthy/expensive design and permit process\",\n",
    "    9: \"lack of enough trained personnel in DC systems\",\n",
    "    10: \"uncertain regulatory roadmap\",\n",
    "    11: \"high costs of DC solutions\",\n",
    "}\n",
    "\n",
    "CANON_FIRST5 = set(list(BARRIERS.keys())[:5])\n",
    "\n",
    "# ---------------- Assumes df is already in memory ----------------\n",
    "# Required columns: base_model, variant_id, model, barrier_id\n",
    "# Optional but harmless: bias_type, iteration, timestamp, etc.\n",
    "\n",
    "# Light typing / cleaning\n",
    "df = df.copy()\n",
    "df[\"base_model\"] = df[\"base_model\"].astype(str).str.strip()\n",
    "df[\"variant_id\"] = df[\"variant_id\"].astype(str).str.strip()\n",
    "df[\"model\"]      = df[\"model\"].astype(str).str.strip()\n",
    "df[\"barrier_id\"] = pd.to_numeric(df[\"barrier_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# ---------------- 1) Selection counts & rates per combo ----------------\n",
    "GROUP = [\"base_model\", \"variant_id\", \"model\"]\n",
    "GROUP_BASE =[\"base_model\"]\n",
    "\n",
    "\n",
    "# --- canonical first 5 from dict insertion order ---\n",
    "CANON_FIRST5_LIST = list(BARRIERS.keys())[:5]\n",
    "CANON_FIRST5 = set(CANON_FIRST5_LIST)\n",
    "\n",
    "def jaccard(a: set, b: set) -> float:\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    return len(a & b) / len(a | b) if (a or b) else 0.0\n",
    "\n",
    "counts_base = (\n",
    "    df.dropna(subset=[\"barrier_id\"])\n",
    "      .groupby(GROUP_BASE + [\"barrier_id\"], dropna=False)\n",
    "      .size()\n",
    "        .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "totals_base = (\n",
    "    counts_base.groupby(GROUP_BASE, dropna=False)[\"count\"]\n",
    "            .sum()\n",
    "            .reset_index(name=\"total_count\")\n",
    ")\n",
    "\n",
    "#top-5 by aggregated counts\n",
    "sorted_counts_base = counts_base.sort_values(\n",
    "    GROUP_BASE + [\"count\", \"barrier_id\"],\n",
    "    ascending=[True]*len(GROUP_BASE) + [False, True]\n",
    ")\n",
    "\n",
    "top5_rows_base=(\n",
    "    sorted_counts_base\n",
    "    .groupby(GROUP_BASE, dropna=False)\n",
    "    .head(5)\n",
    ")\n",
    "\n",
    "top5_ordered_base = (\n",
    "    top5_rows_base.groupby(GROUP_BASE, dropna=False)[\"barrier_id\"]\n",
    "                  .apply(lambda s: [int(x) for x in s.tolist()])   # keep order\n",
    "                  .reset_index(name=\"top5_selected\")\n",
    ")\n",
    "\n",
    "# Union of all selected barriers across variants for each base\n",
    "unique_sets_base = (\n",
    "    df.dropna(subset=[\"barrier_id\"])\n",
    "      .groupby(GROUP_BASE, dropna=False)[\"barrier_id\"]\n",
    "      .apply(lambda s: set(int(x) for x in s.dropna().tolist()))\n",
    "      .reset_index(name=\"selected_set\")\n",
    ")\n",
    "unique_sets_base[\"n_selected_unique\"] = unique_sets_base[\"selected_set\"].map(len)\n",
    "\n",
    "# Assemble base-level output (same columns schema)\n",
    "jaccard_out_base = (\n",
    "    totals_base\n",
    "    .merge(unique_sets_base, on=GROUP_BASE, how=\"left\")\n",
    "    .merge(top5_ordered_base, on=GROUP_BASE, how=\"left\")\n",
    "    .assign(\n",
    "        jaccard_top5_vs_first5=lambda d: d[\"top5_selected\"].map(lambda lst: jaccard(set(lst or []), set(CANON_FIRST5_LIST))),\n",
    "        # readable versions\n",
    "        selected_set=lambda d: d[\"selected_set\"].map(lambda s: sorted(list(s)) if isinstance(s, set) else []),\n",
    "        canon_first5=[CANON_FIRST5_LIST]*len(totals_base)\n",
    "    )[\n",
    "        [\"base_model\",\n",
    "         \"total_count\",\n",
    "         \"n_selected_unique\", \"selected_set\",\n",
    "         \"top5_selected\", \"jaccard_top5_vs_first5\", \"canon_first5\"]\n",
    "    ]\n",
    "    .sort_values(\"base_model\")\n",
    ")\n",
    "\n",
    "jaccard_out_base.to_csv(\"ordered_list/orderedlist_jaccard_compressed.csv\", index=False)\n",
    "\n",
    "print(\" - ordered_list/orderedlist_jaccard_compressed.csv\")\n",
    "\n",
    "# All barrier ids (to fix a consistent column order)\n",
    "barrier_ids_sorted = sorted(int(x) for x in df[\"barrier_id\"].dropna().unique())\n",
    "\n",
    "# Aggregated counts per base (combine all variants)\n",
    "counts_base = (\n",
    "    df.dropna(subset=[\"barrier_id\"])\n",
    "      .groupby(GROUP_BASE + [\"barrier_id\"], dropna=False)\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# Total selections per base\n",
    "totals_base = (\n",
    "    counts_base.groupby(GROUP_BASE, dropna=False)[\"count\"]\n",
    "               .sum()\n",
    "               .reset_index(name=\"total_count\")\n",
    ")\n",
    "\n",
    "# ---- COUNTS wide (one row per base, one column per barrier) ----\n",
    "counts_base_wide = (\n",
    "    counts_base.pivot_table(index=GROUP_BASE, columns=\"barrier_id\", values=\"count\", fill_value=0)\n",
    "               .rename_axis(None, axis=1)\n",
    "               .reset_index()\n",
    ")\n",
    "\n",
    "# Ensure barrier columns appear in numeric order B1..B* (as raw ints, same as your original)\n",
    "ordered_cols_counts = [\"base_model\"] + barrier_ids_sorted\n",
    "# Add missing columns (if any barriers absent for all bases)\n",
    "for c in barrier_ids_sorted:\n",
    "    if c not in counts_base_wide.columns:\n",
    "        counts_base_wide[c] = 0\n",
    "counts_base_wide = counts_base_wide.reindex(columns=ordered_cols_counts)\n",
    "\n",
    "# Append total_count\n",
    "counts_base_wide = counts_base_wide.merge(totals_base, on=\"base_model\", how=\"left\")\n",
    "\n",
    "# ---- RATES wide (counts / total_count per base) ----\n",
    "rate_base = counts_base.merge(totals_base, on=\"base_model\", how=\"left\")\n",
    "rate_base[\"rate\"] = rate_base[\"count\"] / rate_base[\"total_count\"].replace(0, np.nan)\n",
    "\n",
    "rates_base_wide = (\n",
    "    rate_base.pivot_table(index=GROUP_BASE, columns=\"barrier_id\", values=\"rate\", fill_value=0.0)\n",
    "             .rename_axis(None, axis=1)\n",
    "             .reset_index()\n",
    ")\n",
    "\n",
    "# Ensure same barrier column order\n",
    "ordered_cols_rates = [\"base_model\"] + barrier_ids_sorted\n",
    "for c in barrier_ids_sorted:\n",
    "    if c not in rates_base_wide.columns:\n",
    "        rates_base_wide[c] = 0.0\n",
    "rates_base_wide = rates_base_wide.reindex(columns=ordered_cols_rates)\n",
    "\n",
    "# Append total_count for convenience/verification\n",
    "rates_base_wide = rates_base_wide.merge(totals_base, on=\"base_model\", how=\"left\")\n",
    "\n",
    "# ---- Write CSVs ----\n",
    "counts_base_wide.to_csv(\"ordered_list/orderedlist_selection_counts_compressed.csv\", index=False)\n",
    "rates_base_wide.to_csv(\"ordered_list/orderedlist_selection_rates_compressed.csv\", index=False)\n",
    "\n",
    "print(\" - ordered_list/orderedlist_selection_counts_compressed.csv\")\n",
    "print(\" - ordered_list/orderedlist_selection_rates_compressed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6feb453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote:\n",
      " - unnumbered_list/unnumberedlist_selection_counts_by_combo_wide.csv\n",
      " - unnumbered_list/unnumberedlist_selection_rates_by_combo_wide.csv\n",
      " - unnumbered_list/unnumberedlist_jaccard_vs_canonical_first5_by_combo.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"unnumbered_list/unnumbered_barriers_max5_clean.csv\")\n",
    "\n",
    "# ---------------- Canonical barrier list (source of truth) ----------------\n",
    "BARRIERS = {\n",
    "    1: \"power losses, quality and safety issues\",\n",
    "    2: \"reduced reliability in DC devices\",\n",
    "    3: \"lack of use-cases in which DC is advantageous\",\n",
    "    4: \"uncertain utility interaction (net metering, utility ownership, and agreed standards)\",\n",
    "    5: \"lack of pilot projects\",\n",
    "    6: \"public perception of DC and readiness to 'champion' installations from DC projects\",\n",
    "    7: \"incompatibility of DC systems components\",\n",
    "    8: \"misconception and lack of knowledge leads to lengthy/expensive design and permit process\",\n",
    "    9: \"lack of enough trained personnel in DC systems\",\n",
    "    10: \"uncertain regulatory roadmap\",\n",
    "    11: \"high costs of DC solutions\",\n",
    "}\n",
    "\n",
    "CANON_FIRST5 = set(list(BARRIERS.keys())[:5])\n",
    "\n",
    "# ---------------- Assumes df is already in memory ----------------\n",
    "# Required columns: base_model, variant_id, model, barrier_id\n",
    "# Optional but harmless: bias_type, iteration, timestamp, etc.\n",
    "\n",
    "# Light typing / cleaning\n",
    "df = df.copy()\n",
    "df[\"base_model\"] = df[\"base_model\"].astype(str).str.strip()\n",
    "df[\"variant_id\"] = df[\"variant_id\"].astype(str).str.strip()\n",
    "df[\"model\"]      = df[\"model\"].astype(str).str.strip()\n",
    "df[\"barrier_id\"] = pd.to_numeric(df[\"barrier_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# ---------------- 1) Selection counts & rates per combo ----------------\n",
    "GROUP = [\"base_model\", \"variant_id\", \"model\"]\n",
    "\n",
    "counts = (\n",
    "    df.dropna(subset=[\"barrier_id\"])\n",
    "      .groupby(GROUP + [\"barrier_id\"], dropna=False)\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "totals = counts.groupby(GROUP, dropna=False)[\"count\"].sum().reset_index(name=\"total_count\")\n",
    "\n",
    "counts = counts.merge(totals, on=GROUP, how=\"left\")\n",
    "counts[\"rate\"] = counts[\"count\"] / counts[\"total_count\"].replace(0, np.nan)\n",
    "\n",
    "# Wide tables: one column per barrier_id (counts and rates)\n",
    "counts_wide = (\n",
    "    counts.pivot_table(index=GROUP, columns=\"barrier_id\", values=\"count\", fill_value=0)\n",
    "          .rename_axis(None, axis=1)\n",
    "          .reset_index()\n",
    ")\n",
    "rates_wide = (\n",
    "    counts.pivot_table(index=GROUP, columns=\"barrier_id\", values=\"rate\", fill_value=0.0)\n",
    "          .rename_axis(None, axis=1)\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "# Add total_count to both for convenience\n",
    "counts_wide = counts_wide.merge(totals, on=GROUP, how=\"left\")\n",
    "rates_wide  = rates_wide.merge(totals, on=GROUP, how=\"left\")\n",
    "\n",
    "# Save selection tables\n",
    "counts_wide.to_csv(\"unnumbered_list/unnumberedlist_selection_counts_by_combo_wide.csv\", index=False)\n",
    "rates_wide.to_csv(\"unnumbered_list/unnumberedlist_selection_rates_by_combo_wide.csv\", index=False)\n",
    "\n",
    "# --- canonical first 5 from dict insertion order ---\n",
    "CANON_FIRST5_LIST = list(BARRIERS.keys())[:5]\n",
    "CANON_FIRST5 = set(CANON_FIRST5_LIST)\n",
    "\n",
    "def jaccard(a: set, b: set) -> float:\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    return len(a & b) / len(a | b) if (a or b) else 0.0\n",
    "\n",
    "# --- build top-5 per GROUP: both set and ordered list ---\n",
    "# sort by GROUP (asc) then count (desc)\n",
    "sorted_counts = counts.sort_values(\n",
    "    GROUP + [\"count\"],\n",
    "    ascending=[True]*len(GROUP) + [False]\n",
    ")\n",
    "\n",
    "# take top-5 rows per combo\n",
    "top5_rows = (\n",
    "    sorted_counts\n",
    "    .groupby(GROUP, dropna=False)\n",
    "    .head(5)\n",
    ")\n",
    "\n",
    "# ordered list (by count desc) and set for Jaccard\n",
    "top5_ordered = (\n",
    "    top5_rows.groupby(GROUP, dropna=False)[\"barrier_id\"]\n",
    "             .apply(lambda s: [int(x) for x in s.tolist()])           # keep order\n",
    "             .reset_index(name=\"top5_selected\")                        # ordered list\n",
    ")\n",
    "\n",
    "top5_sets = (\n",
    "    top5_rows.groupby(GROUP, dropna=False)[\"barrier_id\"]\n",
    "             .apply(lambda s: set(int(x) for x in s.dropna().tolist()))\n",
    "             .reset_index(name=\"top5_set\")                             # set for Jaccard\n",
    ")\n",
    "\n",
    "top5 = top5_ordered.merge(top5_sets, on=GROUP, how=\"left\")\n",
    "top5[\"jaccard_top5_vs_first5\"] = top5[\"top5_set\"].map(lambda S: jaccard(S, CANON_FIRST5))\n",
    "\n",
    "# (optional) unique-set section can stay or be removed; keeping here for context\n",
    "unique_sets = (\n",
    "    df.dropna(subset=[\"barrier_id\"])\n",
    "      .groupby(GROUP, dropna=False)[\"barrier_id\"]\n",
    "      .apply(lambda s: set(int(x) for x in s.dropna().tolist()))\n",
    "      .reset_index(name=\"selected_set\")\n",
    ")\n",
    "unique_sets[\"n_selected_unique\"] = unique_sets[\"selected_set\"].map(len)\n",
    "\n",
    "# --- assemble output with easy-to-audit columns ---\n",
    "jaccard_out = (\n",
    "    totals\n",
    "    .merge(unique_sets, on=GROUP, how=\"left\")\n",
    "    .merge(top5,        on=GROUP, how=\"left\")\n",
    "    .assign(\n",
    "        # readable versions\n",
    "        selected_set=lambda d: d[\"selected_set\"].map(lambda s: sorted(list(s)) if isinstance(s, set) else []),\n",
    "        canon_first5=lambda d: [CANON_FIRST5_LIST]*len(d)\n",
    "    )[\n",
    "        GROUP\n",
    "        + [\"total_count\",\n",
    "           \"n_selected_unique\", \"selected_set\",\n",
    "           \"top5_selected\", \"jaccard_top5_vs_first5\", \"canon_first5\"]\n",
    "    ]\n",
    "    .sort_values(GROUP)\n",
    ")\n",
    "\n",
    "jaccard_out.to_csv(\"unnumbered_list/unnumberedlist_jaccard_vs_canonical_first5_by_combo.csv\", index=False)\n",
    "\n",
    "\n",
    "print(\"✅ Wrote:\")\n",
    "print(\" - unnumbered_list/unnumberedlist_selection_counts_by_combo_wide.csv\")\n",
    "print(\" - unnumbered_list/unnumberedlist_selection_rates_by_combo_wide.csv\")\n",
    "print(\" - unnumbered_list/unnumberedlist_jaccard_vs_canonical_first5_by_combo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2de18fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - unnumbered_list/unnumberedlist_jaccard_compressed.csv\n",
      " - unnumbered_list/unnumberedlist_selection_counts_compressed.csv\n",
      " - unnumbered_list/unnumberedlist_selection_rates_compressed.csv\n"
     ]
    }
   ],
   "source": [
    "#COMPRESSED FILES UNNUMBERED BARRIER LIST\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"unnumbered_list/unnumbered_barriers_max5_clean.csv\")\n",
    "\n",
    "# ---------------- Canonical barrier list (source of truth) ----------------\n",
    "BARRIERS = {\n",
    "    1: \"power losses, quality and safety issues\",\n",
    "    2: \"reduced reliability in DC devices\",\n",
    "    3: \"lack of use-cases in which DC is advantageous\",\n",
    "    4: \"uncertain utility interaction (net metering, utility ownership, and agreed standards)\",\n",
    "    5: \"lack of pilot projects\",\n",
    "    6: \"public perception of DC and readiness to 'champion' installations from DC projects\",\n",
    "    7: \"incompatibility of DC systems components\",\n",
    "    8: \"misconception and lack of knowledge leads to lengthy/expensive design and permit process\",\n",
    "    9: \"lack of enough trained personnel in DC systems\",\n",
    "    10: \"uncertain regulatory roadmap\",\n",
    "    11: \"high costs of DC solutions\",\n",
    "}\n",
    "\n",
    "CANON_FIRST5 = set(list(BARRIERS.keys())[:5])\n",
    "\n",
    "# ---------------- Assumes df is already in memory ----------------\n",
    "# Required columns: base_model, variant_id, model, barrier_id\n",
    "# Optional but harmless: bias_type, iteration, timestamp, etc.\n",
    "\n",
    "# Light typing / cleaning\n",
    "df = df.copy()\n",
    "df[\"base_model\"] = df[\"base_model\"].astype(str).str.strip()\n",
    "df[\"variant_id\"] = df[\"variant_id\"].astype(str).str.strip()\n",
    "df[\"model\"]      = df[\"model\"].astype(str).str.strip()\n",
    "df[\"barrier_id\"] = pd.to_numeric(df[\"barrier_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# ---------------- 1) Selection counts & rates per combo ----------------\n",
    "GROUP = [\"base_model\", \"variant_id\", \"model\"]\n",
    "GROUP_BASE =[\"base_model\"]\n",
    "\n",
    "\n",
    "# --- canonical first 5 from dict insertion order ---\n",
    "CANON_FIRST5_LIST = list(BARRIERS.keys())[:5]\n",
    "CANON_FIRST5 = set(CANON_FIRST5_LIST)\n",
    "\n",
    "def jaccard(a: set, b: set) -> float:\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    return len(a & b) / len(a | b) if (a or b) else 0.0\n",
    "\n",
    "counts_base = (\n",
    "    df.dropna(subset=[\"barrier_id\"])\n",
    "      .groupby(GROUP_BASE + [\"barrier_id\"], dropna=False)\n",
    "      .size()\n",
    "        .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "totals_base = (\n",
    "    counts_base.groupby(GROUP_BASE, dropna=False)[\"count\"]\n",
    "            .sum()\n",
    "            .reset_index(name=\"total_count\")\n",
    ")\n",
    "\n",
    "#top-5 by aggregated counts\n",
    "sorted_counts_base = counts_base.sort_values(\n",
    "    GROUP_BASE + [\"count\", \"barrier_id\"],\n",
    "    ascending=[True]*len(GROUP_BASE) + [False, True]\n",
    ")\n",
    "\n",
    "top5_rows_base=(\n",
    "    sorted_counts_base\n",
    "    .groupby(GROUP_BASE, dropna=False)\n",
    "    .head(5)\n",
    ")\n",
    "\n",
    "top5_ordered_base = (\n",
    "    top5_rows_base.groupby(GROUP_BASE, dropna=False)[\"barrier_id\"]\n",
    "                  .apply(lambda s: [int(x) for x in s.tolist()])   # keep order\n",
    "                  .reset_index(name=\"top5_selected\")\n",
    ")\n",
    "\n",
    "# Union of all selected barriers across variants for each base\n",
    "unique_sets_base = (\n",
    "    df.dropna(subset=[\"barrier_id\"])\n",
    "      .groupby(GROUP_BASE, dropna=False)[\"barrier_id\"]\n",
    "      .apply(lambda s: set(int(x) for x in s.dropna().tolist()))\n",
    "      .reset_index(name=\"selected_set\")\n",
    ")\n",
    "unique_sets_base[\"n_selected_unique\"] = unique_sets_base[\"selected_set\"].map(len)\n",
    "\n",
    "# Assemble base-level output (same columns schema)\n",
    "jaccard_out_base = (\n",
    "    totals_base\n",
    "    .merge(unique_sets_base, on=GROUP_BASE, how=\"left\")\n",
    "    .merge(top5_ordered_base, on=GROUP_BASE, how=\"left\")\n",
    "    .assign(\n",
    "        jaccard_top5_vs_first5=lambda d: d[\"top5_selected\"].map(lambda lst: jaccard(set(lst or []), set(CANON_FIRST5_LIST))),\n",
    "        # readable versions\n",
    "        selected_set=lambda d: d[\"selected_set\"].map(lambda s: sorted(list(s)) if isinstance(s, set) else []),\n",
    "        canon_first5=[CANON_FIRST5_LIST]*len(totals_base)\n",
    "    )[\n",
    "        [\"base_model\",\n",
    "         \"total_count\",\n",
    "         \"n_selected_unique\", \"selected_set\",\n",
    "         \"top5_selected\", \"jaccard_top5_vs_first5\", \"canon_first5\"]\n",
    "    ]\n",
    "    .sort_values(\"base_model\")\n",
    ")\n",
    "\n",
    "jaccard_out_base.to_csv(\"unnumbered_list/unnumberedlist_jaccard_compressed.csv\", index=False)\n",
    "\n",
    "print(\" - unnumbered_list/unnumberedlist_jaccard_compressed.csv\")\n",
    "\n",
    "# All barrier ids (to fix a consistent column order)\n",
    "barrier_ids_sorted = sorted(int(x) for x in df[\"barrier_id\"].dropna().unique())\n",
    "\n",
    "# Aggregated counts per base (combine all variants)\n",
    "counts_base = (\n",
    "    df.dropna(subset=[\"barrier_id\"])\n",
    "      .groupby(GROUP_BASE + [\"barrier_id\"], dropna=False)\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# Total selections per base\n",
    "totals_base = (\n",
    "    counts_base.groupby(GROUP_BASE, dropna=False)[\"count\"]\n",
    "               .sum()\n",
    "               .reset_index(name=\"total_count\")\n",
    ")\n",
    "\n",
    "# ---- COUNTS wide (one row per base, one column per barrier) ----\n",
    "counts_base_wide = (\n",
    "    counts_base.pivot_table(index=GROUP_BASE, columns=\"barrier_id\", values=\"count\", fill_value=0)\n",
    "               .rename_axis(None, axis=1)\n",
    "               .reset_index()\n",
    ")\n",
    "\n",
    "# Ensure barrier columns appear in numeric order B1..B* (as raw ints, same as your original)\n",
    "ordered_cols_counts = [\"base_model\"] + barrier_ids_sorted\n",
    "# Add missing columns (if any barriers absent for all bases)\n",
    "for c in barrier_ids_sorted:\n",
    "    if c not in counts_base_wide.columns:\n",
    "        counts_base_wide[c] = 0\n",
    "counts_base_wide = counts_base_wide.reindex(columns=ordered_cols_counts)\n",
    "\n",
    "# Append total_count\n",
    "counts_base_wide = counts_base_wide.merge(totals_base, on=\"base_model\", how=\"left\")\n",
    "\n",
    "# ---- RATES wide (counts / total_count per base) ----\n",
    "rate_base = counts_base.merge(totals_base, on=\"base_model\", how=\"left\")\n",
    "rate_base[\"rate\"] = rate_base[\"count\"] / rate_base[\"total_count\"].replace(0, np.nan)\n",
    "\n",
    "rates_base_wide = (\n",
    "    rate_base.pivot_table(index=GROUP_BASE, columns=\"barrier_id\", values=\"rate\", fill_value=0.0)\n",
    "             .rename_axis(None, axis=1)\n",
    "             .reset_index()\n",
    ")\n",
    "\n",
    "# Ensure same barrier column order\n",
    "ordered_cols_rates = [\"base_model\"] + barrier_ids_sorted\n",
    "for c in barrier_ids_sorted:\n",
    "    if c not in rates_base_wide.columns:\n",
    "        rates_base_wide[c] = 0.0\n",
    "rates_base_wide = rates_base_wide.reindex(columns=ordered_cols_rates)\n",
    "\n",
    "# Append total_count for convenience/verification\n",
    "rates_base_wide = rates_base_wide.merge(totals_base, on=\"base_model\", how=\"left\")\n",
    "\n",
    "# ---- Write CSVs ----\n",
    "counts_base_wide.to_csv(\"unnumbered_list/unnumberedlist_selection_counts_compressed.csv\", index=False)\n",
    "rates_base_wide.to_csv(\"unnumbered_list/unnumberedlist_selection_rates_compressed.csv\", index=False)\n",
    "\n",
    "print(\" - unnumbered_list/unnumberedlist_selection_counts_compressed.csv\")\n",
    "print(\" - unnumbered_list/unnumberedlist_selection_rates_compressed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
