{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04368d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10800, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"merged_barrier_bianeu_llmhuman.csv\")\n",
    "\n",
    "#drop rows which 'source'=human\n",
    "df=df[df['source']!='human']\n",
    "\n",
    "#save to csv\n",
    "df.to_csv(\"merged_barrier_biasneu_no_human.csv\",index=False)\n",
    "\n",
    "#change where 'condition'=BIASED to 'ANCHOR'\n",
    "df['condition']=df['condition'].replace('BIASED','ANCHOR')\n",
    "\n",
    "#save to csv\n",
    "df.to_csv(\"merged_barrier_anchorneu-no_human.csv\",index=False)\n",
    "\n",
    "#change where 'condition'='NEU' to 'CONTEXT'\n",
    "df['condition']=df['condition'].replace('NEU','CONTEXT')\n",
    "\n",
    "#save to csv\n",
    "df.to_csv(\"merged_barrier_anchorctx-no_human.csv\",index=False)\n",
    "\n",
    "#change where 'scenario'=BIASED to 'ANCHOR'\n",
    "df['scenario']=df['scenario'].replace('BIASED','ANCHOR')\n",
    "\n",
    "#save to csv\n",
    "df.to_csv(\"merged_barrier_anchorneu-scenarioanchor.csv\",index=False)\n",
    "\n",
    "#change where 'scenario'='NEU' to 'CONTEXT'\n",
    "df['scenario']=df['scenario'].replace('NEU','CONTEXT')\n",
    "\n",
    "#save to csv\n",
    "df.to_csv(\"merged_barrier_anchorctx-scenario_anchorcontext.csv\",index=False)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9018a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# csv\n",
    "df=pd.read_csv(\"merged_barrier_anchorctx-scenario_anchorcontext.csv\")\n",
    "\n",
    "\n",
    "#change 'bias_type' from 'BIAS_*'   to 'ANCHOR_*'\n",
    "df['bias_type']=df['bias_type'].replace('BIAS_WORD','ANCHOR_WORD')\n",
    "df['bias_type']=df['bias_type'].replace('BIAS_EXAMPLE','ANCHOR_EXAMPLE')\n",
    "df['bias_type']=df['bias_type'].replace('BIAS_NUM_LOW','ANCHOR_NUM_LOW')\n",
    "df['bias_type']=df['bias_type'].replace('BIAS_NUM_HIGH','ANCHOR_NUM_HIGH')\n",
    "\n",
    "#save to csv\n",
    "df.to_csv(\"merged_barrier_anchorctx-scenario_anchortypes.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bba2ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# csv\n",
    "df=pd.read_csv(\"merged_barrier_anchorctx-scenario_anchortypes.csv\")\n",
    "\n",
    "mapping = {\n",
    "    \"BIAS_WORD\":     \"ANCHOR_WORD\",\n",
    "    \"BIAS_EXAMPLE\":  \"ANCHOR_EXAMPLE\",\n",
    "    \"BIAS_NUM_LOW\":  \"ANCHOR_LOW_NUM\",\n",
    "    \"BIAS_NUM_HIGH\": \"ANCHOR_HIGH_NUM\",\n",
    "}\n",
    "\n",
    "# Plain string\n",
    "def replace_bias_prefix(s: str) -> str:\n",
    "    return re.sub(\n",
    "        r\"^(BIAS_WORD|BIAS_EXAMPLE|BIAS_NUM_LOW|BIAS_NUM_HIGH)\",\n",
    "        lambda m: mapping[m.group(1)],\n",
    "        s,\n",
    "    )\n",
    "\n",
    "# Pandas column\n",
    "df['row_id'] = df['row_id'].apply(replace_bias_prefix)\n",
    "\n",
    "#save to csv\n",
    "df.to_csv(\"merged_barrier_anchorctx-scenario_anchortypes_rowid_fixed.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f48d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap\n",
    "import re\n",
    "\n",
    "df=pd.read_csv(\"merged_barrier_anchorctx-scenario_anchortypes_rowid_fixed.csv\")\n",
    "\n",
    "# --- Canonical mapping (your source of truth) ---\n",
    "BARRIERS = {\n",
    "    1: \"power losses, quality and safety issues\",\n",
    "    2: \"reduced reliability in DC devices\",\n",
    "    3: \"lack of use-cases in which DC is advantageous\",\n",
    "    4: \"uncertain utility interaction (net metering, utility ownership, and agreed standards)\",\n",
    "    5: \"lack of pilot projects\",\n",
    "    6: \"public perception of DC and readiness to 'champion' installations from DC projects\",\n",
    "    7: \"incompatibility of DC systems components\",\n",
    "    8: \"misconception and lack of knowledge leads to lengthy/expensive design and permit process\",\n",
    "    9: \"lack of enough trained personnel in DC systems\",\n",
    "    10: \"uncertain regulatory roadmap\",\n",
    "    11: \"high costs of DC solutions\",\n",
    "}\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    s = str(s).lower().strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)                     # collapse whitespace\n",
    "    s = re.sub(r\"[^\\w\\s']\", \" \", s)                # drop punctuation except apostrophes\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# --- Main validator ---\n",
    "def validate_barriers(df: pd.DataFrame,\n",
    "                      id_col: str = \"barrier_id\",\n",
    "                      text_col: str | None = \"barrier_label\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Checks:\n",
    "      1) All IDs present in df are valid keys of BARRIERS.\n",
    "      2) (Optional) If text_col is provided, compare normalized text to canonical text.\n",
    "    Returns a per-row report with flags.\n",
    "    \"\"\"\n",
    "    # Coerce IDs to integers where possible\n",
    "    ids = pd.to_numeric(df[id_col], errors=\"coerce\").astype(\"Int64\")\n",
    "    rep = df.copy()\n",
    "    rep[id_col] = ids\n",
    "\n",
    "    # Map expected canonical text\n",
    "    canon_df = pd.DataFrame.from_dict(BARRIERS, orient=\"index\", columns=[\"expected_text\"]).rename_axis(id_col).reset_index()\n",
    "    rep = rep.merge(canon_df, on=id_col, how=\"left\")\n",
    "\n",
    "    # Flags\n",
    "    rep[\"id_known\"] = rep[\"expected_text\"].notna()\n",
    "\n",
    "    if text_col is not None and text_col in rep.columns:\n",
    "        rep[\"given_text_norm\"]   = rep[text_col].map(_norm)\n",
    "        rep[\"expected_text_norm\"]= rep[\"expected_text\"].map(_norm)\n",
    "        rep[\"text_matches\"]      = np.where(rep[\"id_known\"], rep[\"given_text_norm\"] == rep[\"expected_text_norm\"], pd.NA)\n",
    "    else:\n",
    "        rep[\"text_matches\"] = pd.NA\n",
    "\n",
    "    # Summary prints\n",
    "    invalid_ids = rep.loc[~rep[\"id_known\"], id_col].dropna().unique()\n",
    "    if len(invalid_ids):\n",
    "        print(f\"❌ Unknown barrier_id values found: {sorted(map(int, invalid_ids))}\")\n",
    "    else:\n",
    "        print(\"✅ All barrier_id values are valid.\")\n",
    "\n",
    "    if text_col and text_col in rep.columns:\n",
    "        mism = rep[(rep[\"id_known\"]) & (rep[\"text_matches\"] == False)]\n",
    "        if len(mism):\n",
    "            print(f\"❌ {len(mism)} rows where {text_col} doesn't match the canonical text.\")\n",
    "        else:\n",
    "            print(f\"✅ All known IDs have matching {text_col} (after normalization).\")\n",
    "\n",
    "    # Useful quick stats\n",
    "    print(\"\\nCounts by barrier_id in your data:\")\n",
    "    print(rep[\"barrier_id\"].value_counts(dropna=False).sort_index())\n",
    "\n",
    "    return rep\n",
    "# Run the validator (adjust column names if needed)\n",
    "report = validate_barriers(df, id_col=\"barrier_id\", text_col=\"barrier_text\")\n",
    "\n",
    "# Save full report\n",
    "report.to_csv(\"barrier_validation_report.csv\", index=False)\n",
    "\n",
    "# (Optional) Save only problematic rows\n",
    "bad_rows = report[(report[\"id_known\"] == False) | (report[\"text_matches\"] == False)]\n",
    "bad_rows.to_csv(\"barrier_validation_issues.csv\", index=False)\n",
    "\n",
    "# (Optional) quick peek in notebook\n",
    "report.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde4651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"merged_barrier_anchorctx-scenario_anchortypes_rowid_fixed.csv\")\n",
    "\n",
    "df.info()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eed9446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_4944\\3620546462.py:78: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[True True True ... True True True]' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  count_ok[is_context] = grp.loc[is_context, \"rows\"] <= MAX_PER_ITER_CONTEXT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved:\n",
      " - barriers_verified_rows.csv (rows in groups that passed)\n",
      " - barriers_violations_rows.csv (rows in groups that failed)\n",
      " - barriers_group_summary.csv (one line per group with flags)\n",
      " - barriers_violations_summary.csv (failed groups with diagnostics)\n",
      "\n",
      "Variants per base_model:\n",
      " base_model\n",
      "gemma3     12\n",
      "llama      12\n",
      "mistral    12\n",
      "phi4       12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv(\"merged_barrier_anchorctx-scenario_anchortypes_rowid_fixed.csv\")\n",
    "# ---- Canonical barriers ----\n",
    "BARRIERS = {\n",
    "    1: \"power losses, quality and safety issues\",\n",
    "    2: \"reduced reliability in DC devices\",\n",
    "    3: \"lack of use-cases in which DC is advantageous\",\n",
    "    4: \"uncertain utility interaction (net metering, utility ownership, and agreed standards)\",\n",
    "    5: \"lack of pilot projects\",\n",
    "    6: \"public perception of DC and readiness to 'champion' installations from DC projects\",\n",
    "    7: \"incompatibility of DC systems components\",\n",
    "    8: \"misconception and lack of knowledge leads to lengthy/expensive design and permit process\",\n",
    "    9: \"lack of enough trained personnel in DC systems\",\n",
    "    10: \"uncertain regulatory roadmap\",\n",
    "    11: \"high costs of DC solutions\"\n",
    "}\n",
    "valid_ids = set(BARRIERS.keys())\n",
    "\n",
    "EXPECTED_PER_ITER_ANCHOR = 5  # anchors must be exactly 5\n",
    "MAX_PER_ITER_CONTEXT     = 5  # context must be <= 5\n",
    "\n",
    "# ---- df should already be loaded; if not: df = pd.read_csv(\"your_file.csv\")\n",
    "df = df.copy()\n",
    "\n",
    "# Normalize key fields\n",
    "df[\"base_model\"] = df[\"base_model\"].astype(str).str.strip()\n",
    "df[\"variant_id\"] = df[\"variant_id\"].astype(str).str.strip()\n",
    "\n",
    "# condition can be 'ANCHOR' or 'CONTEXT'\n",
    "df[\"condition\"] = df[\"condition\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "# bias_type holds the anchor arm (if present)\n",
    "if \"bias_type\" in df.columns:\n",
    "    df[\"bias_type\"] = df[\"bias_type\"].astype(str).str.strip().str.upper().replace({\"\": np.nan})\n",
    "else:\n",
    "    df[\"bias_type\"] = np.nan\n",
    "\n",
    "# iteration numeric\n",
    "df[\"iteration\"] = pd.to_numeric(df[\"iteration\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# barrier_id numeric Int64\n",
    "df[\"barrier_id\"] = pd.to_numeric(df[\"barrier_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Build the arm for grouping: CONTEXT vs anchor arm\n",
    "# If condition==CONTEXT -> \"CONTEXT\"\n",
    "# Else use bias_type if available; otherwise fallback to condition (will pool anchors together if bias_type missing)\n",
    "df[\"arm\"] = np.where(\n",
    "    df[\"condition\"] == \"CONTEXT\",\n",
    "    \"CONTEXT\",\n",
    "    np.where(df[\"bias_type\"].notna(), df[\"bias_type\"], df[\"condition\"])\n",
    ")\n",
    "\n",
    "GROUP_COLS = [\"base_model\", \"variant_id\", \"arm\", \"iteration\"]\n",
    "\n",
    "# ---- Per-group aggregation\n",
    "grp = (\n",
    "    df.groupby(GROUP_COLS, dropna=False)\n",
    "      .agg(\n",
    "          rows=(\"row_id\", \"size\"),\n",
    "          n_unique_ids=(\"barrier_id\", lambda s: s.dropna().nunique()),\n",
    "          any_na_ids=(\"barrier_id\", lambda s: s.isna().any()),\n",
    "          ids_list=(\"barrier_id\", lambda s: tuple(int(x) for x in s.dropna().tolist())),\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# rule per group (vectorized)\n",
    "is_context = grp[\"arm\"].eq(\"CONTEXT\")\n",
    "is_anchor  = ~is_context\n",
    "\n",
    "# Basic constraints\n",
    "count_ok = pd.Series(False, index=grp.index)\n",
    "uniq_ok  = pd.Series(False, index=grp.index)\n",
    "valid_ok = grp[\"ids_list\"].map(lambda ids: all(i in valid_ids for i in ids))\n",
    "\n",
    "# CONTEXT: rows <= 5 and unique == rows (no duplicates)\n",
    "count_ok[is_context] = grp.loc[is_context, \"rows\"] <= MAX_PER_ITER_CONTEXT\n",
    "uniq_ok[is_context]  = grp.loc[is_context, \"n_unique_ids\"] == grp.loc[is_context, \"rows\"]\n",
    "\n",
    "# ANCHOR arms: rows == 5 and unique == 5\n",
    "count_ok[is_anchor] = grp.loc[is_anchor, \"rows\"] == EXPECTED_PER_ITER_ANCHOR\n",
    "uniq_ok[is_anchor]  = grp.loc[is_anchor, \"n_unique_ids\"] == EXPECTED_PER_ITER_ANCHOR\n",
    "\n",
    "grp[\"count_ok\"]    = count_ok\n",
    "grp[\"uniq_ok\"]     = uniq_ok\n",
    "grp[\"valid_ids_ok\"]= valid_ok\n",
    "grp[\"all_ok\"]      = grp[\"count_ok\"] & grp[\"uniq_ok\"] & grp[\"valid_ids_ok\"]\n",
    "\n",
    "# Helpful diagnostics\n",
    "def diag_row(r) -> str:\n",
    "    msgs = []\n",
    "    if r[\"arm\"] == \"CONTEXT\":\n",
    "        if not r[\"count_ok\"]:\n",
    "            msgs.append(f\"CONTEXT count={r['rows']} (max {MAX_PER_ITER_CONTEXT})\")\n",
    "        if not r[\"uniq_ok\"]:\n",
    "            msgs.append(f\"CONTEXT duplicate barrier_ids (unique={r['n_unique_ids']}, rows={r['rows']})\")\n",
    "    else:\n",
    "        if not r[\"count_ok\"]:\n",
    "            msgs.append(f\"ANCHOR count={r['rows']} (expected {EXPECTED_PER_ITER_ANCHOR})\")\n",
    "        if not r[\"uniq_ok\"]:\n",
    "            msgs.append(f\"ANCHOR unique_ids={r['n_unique_ids']} (expected {EXPECTED_PER_ITER_ANCHOR})\")\n",
    "    if not r[\"valid_ids_ok\"]:\n",
    "        bad = [i for i in r[\"ids_list\"] if i not in valid_ids]\n",
    "        if bad:\n",
    "            msgs.append(f\"invalid_ids={bad}\")\n",
    "        else:\n",
    "            msgs.append(\"invalid_ids=True\")\n",
    "    return \"; \".join(msgs) or \"OK\"\n",
    "\n",
    "violations_summary = (\n",
    "    grp[~grp[\"all_ok\"]]\n",
    "    .assign(issue=lambda x: x.apply(diag_row, axis=1))\n",
    "    .sort_values(GROUP_COLS)\n",
    ")\n",
    "\n",
    "# ---- Attach flags back to rows\n",
    "df_flagged = df.merge(grp, on=GROUP_COLS, how=\"left\", validate=\"many_to_one\")\n",
    "\n",
    "verified_rows   = df_flagged[df_flagged[\"all_ok\"]].copy()\n",
    "violations_rows = df_flagged[~df_flagged[\"all_ok\"]].copy()\n",
    "\n",
    "# ---- Save outputs\n",
    "verified_rows.to_csv(\"barriers_verified_rows.csv\", index=False)\n",
    "violations_rows.to_csv(\"barriers_violations_rows.csv\", index=False)\n",
    "grp.to_csv(\"barriers_group_summary.csv\", index=False)\n",
    "violations_summary.to_csv(\"barriers_violations_summary.csv\", index=False)\n",
    "\n",
    "print(\"✅ Saved:\")\n",
    "print(\" - barriers_verified_rows.csv (rows in groups that passed)\")\n",
    "print(\" - barriers_violations_rows.csv (rows in groups that failed)\")\n",
    "print(\" - barriers_group_summary.csv (one line per group with flags)\")\n",
    "print(\" - barriers_violations_summary.csv (failed groups with diagnostics)\")\n",
    "\n",
    "# (Optional) sanity checks for your inventory: 4 base models × 6 variants each\n",
    "inventory = df[[\"base_model\", \"variant_id\"]].drop_duplicates()\n",
    "inv_counts = inventory.groupby(\"base_model\")[\"variant_id\"].nunique().sort_index()\n",
    "print(\"\\nVariants per base_model:\\n\", inv_counts.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3f7d8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"merged_barrier_anchorctx-scenario_anchortypes_rowid_fixed.csv\")\n",
    "#count rows which have condition as 'context'\n",
    "print(df[df['condition'] == 'CONTEXT'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82aa3f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"merged_barrier_anchorctx_final.csv\")\n",
    "#generate new csv with only rows where condition is 'anchor'\n",
    "df[df['condition'] == 'ANCHOR'].to_csv(\"orderedbarrier_anchor.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31b03699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4800, 13)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"orderedbarrier_anchor.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f13866",
   "metadata": {},
   "source": [
    "**this is reserved part for correcting IDs of barriers** so as to save a new csv with clean barrier id ready for anaylsis.\n",
    "\n",
    "GUIDE: \n",
    "The full dataset with all corrected values is in:\n",
    "\n",
    "barriers_corrected_by_text.csv\n",
    "\n",
    "That file contains every original column plus:\n",
    "\n",
    "barrier_correct_id — the ID inferred from official_label\n",
    "\n",
    "barrier_correct_text — canonical text for that ID\n",
    "\n",
    "(optional) id_changed — True where the original barrier_id ≠ barrier_correct_id\n",
    "\n",
    "The other files are just reports:\n",
    "\n",
    "barriers_unmatched_official_label.csv → rows where official_label didn’t match any dictionary entry (so barrier_correct_id is NA)\n",
    "\n",
    "barriers_id_mismatch.csv → rows where original ID disagrees with text-derived ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c6c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# --- Canonical mapping (source of truth) ---\n",
    "BARRIERS = {\n",
    "    1: \"power losses, quality and safety issues\",\n",
    "    2: \"reduced reliability in DC devices\",\n",
    "    3: \"lack of use-cases in which DC is advantageous\",\n",
    "    4: \"uncertain utility interaction (net metering, utility ownership, and agreed standards)\",\n",
    "    5: \"lack of pilot projects\",\n",
    "    6: \"public perception of DC and readiness to 'champion' installations from DC projects\",\n",
    "    7: \"incompatibility of DC systems components\",\n",
    "    8: \"misconception and lack of knowledge leads to lengthy/expensive design and permit process\",\n",
    "    9: \"lack of enough trained personnel in DC systems\",\n",
    "    10: \"uncertain regulatory roadmap\",\n",
    "    11: \"high costs of DC solutions\",\n",
    "}\n",
    "\n",
    "# --- Lightweight normalizer so small formatting differences don't block matches ---\n",
    "def _norm(s: str) -> str:\n",
    "    if s is None or (isinstance(s, float) and np.isnan(s)):\n",
    "        return \"\"\n",
    "    s = str(s).lower().strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)                 # collapse whitespace\n",
    "    s = re.sub(r\"[^\\w\\s']+\", \" \", s)           # drop punctuation except apostrophes\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Reverse map: normalized canonical text -> id\n",
    "rev_map = { _norm(txt): bid for bid, txt in BARRIERS.items() }\n",
    "\n",
    "# Assume you already loaded your CSV as df, and it has a column 'official_label'\n",
    "# If your column name differs, change 'official_label' below accordingly.\n",
    "\n",
    "# 1) Compute corrected id from official_label text\n",
    "df[\"barrier_correct_id\"] = df[\"official_label\"].map(lambda s: rev_map.get(_norm(s), pd.NA)).astype(\"Int64\")\n",
    "\n",
    "# 2) (Optional) canonical text for that id, and quick comparison flags\n",
    "df[\"barrier_correct_text\"] = df[\"barrier_correct_id\"].map(BARRIERS)\n",
    "if \"barrier_id\" in df.columns:\n",
    "    # Flag rows where the original id disagrees with the text-derived id\n",
    "    df[\"barrier_id\"] = pd.to_numeric(df[\"barrier_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"id_changed\"] = (df[\"barrier_id\"] != df[\"barrier_correct_id\"])\n",
    "\n",
    "# 3) Save a full corrected CSV (all original columns + new columns)\n",
    "out_csv = \"barriers_corrected_by_text.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(f\"✅ Wrote: {out_csv}\")\n",
    "\n",
    "# 4) (Optional) Save only rows that could not be matched or that changed\n",
    "unmatched = df[df[\"barrier_correct_id\"].isna()]\n",
    "changed   = df[\"id_changed\"].fillna(False)\n",
    "if len(unmatched):\n",
    "    unmatched.to_csv(\"barriers_unmatched_official_label.csv\", index=False)\n",
    "    print(f\"⚠️  Unmatched rows by text: {len(unmatched)} (saved barriers_unmatched_official_label.csv)\")\n",
    "if \"id_changed\" in df and changed.any():\n",
    "    df.loc[changed].to_csv(\"barriers_id_mismatch.csv\", index=False)\n",
    "    print(f\"⚠️  ID/text mismatches: {changed.sum()} (saved barriers_id_mismatch.csv)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc7d9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ANCHOR' 'CONTEXT']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"merged_barrier_anchorctx_final.csv\")\n",
    "\n",
    "#select unique values at 'condition' column\n",
    "print(df['condition'].unique())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
